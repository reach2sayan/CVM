{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.342725Z",
     "start_time": "2022-01-12T03:44:09.871002Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from jax import jacfwd, jacrev\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from pprint import pprint as pp\n",
    "import re #Used to read chunks from the data files - clusters, vmat, etec.\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import sys\n",
    "from scipy.linalg import eigvals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from scipy.optimize import SR1, BFGS\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "from ase import Atoms\n",
    "from ase.units import kB\n",
    "from ase.visualize import view\n",
    "from ase.build import bulk\n",
    "from ase.spacegroup import crystal\n",
    "from ase.build import make_supercell\n",
    "from ase.io.vasp import write_vasp\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "pattern1 = re.compile(\"\\n\\n\\n\")\n",
    "pattern2 = re.compile(\"\\n\\n\")\n",
    "np.set_printoptions(suppress=True,precision=3)\n",
    "random.seed(42)\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all necessary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read clusters.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.359256Z",
     "start_time": "2022-01-12T03:44:12.350957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read clusters.out\n",
    "clusters = {}\n",
    "\n",
    "with open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/clusters.out','r') as fclusters:\n",
    "    temp_clusters = fclusters.read().split('\\n\\n') #Read blocks separated by 1 empty line\n",
    "\n",
    "for idx, cluster in enumerate(temp_clusters):\n",
    "    if cluster == '': #Check for spurious empty blocks\n",
    "        continue \n",
    "    line = cluster.split('\\n') #If not empty split by lines\n",
    "    multiplicity = int(line[0]) #1st line\n",
    "    length = float(line[1]) #largest distance between two atoms\n",
    "    num_points = int(line[2]) #type of cluster\n",
    "    clusters[idx] = {'mult':multiplicity, 'length':length, 'type':num_points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.428403Z",
     "start_time": "2022-01-12T03:44:12.417274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read config.out\n",
    "configs = {}\n",
    "\n",
    "fconfig = open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/config.out','r')\n",
    "_ = next(fconfig) #Ignore first line\n",
    "\n",
    "temp_config = fconfig.read()#.split('\\n\\n')\n",
    "\n",
    "temp_config = pattern1.split(temp_config) #split lines separated by 2 empty lines\n",
    "\n",
    "for idx, config in enumerate(temp_config):\n",
    "    if config == '': #Check for spurious empty blocks\n",
    "        continue\n",
    "    num_points = int(config.split('\\n')[0]) #number of subclusters\n",
    "    inter = []\n",
    "    config = pattern2.split(config) #now split individual subclusters separated by 1 blank line\n",
    "    for i in range(num_points):\n",
    "        line = config[i].split('\\n')\n",
    "        if i == 0:\n",
    "            length = int(line[1])\n",
    "        else:\n",
    "            length = int(line[0])\n",
    "        tmp_inter = np.array([(list(map(int,l.split(' ')[-2:]))) for l in line[-length:]])\n",
    "        inter.append(tmp_inter)#print(np.array(inter))\n",
    "    configs[idx] = {'inter': inter, 'num_of_subclus': num_points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Kikuchi-Baker coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.448049Z",
     "start_time": "2022-01-12T03:44:12.430824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read kb.out\n",
    "\n",
    "kb = {}\n",
    "fkb = open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/configkb.out','r')\n",
    "_ = next(fkb) #ignore first line\n",
    "\n",
    "temp_kb = fkb.read()\n",
    "temp_kb = temp_kb.split('\\n') #split file linewise\n",
    "\n",
    "for idx, kbcoeff in enumerate(temp_kb):\n",
    "    if kbcoeff == '': #check for spurious empty blocks\n",
    "        continue\n",
    "    kb[idx] = float(kbcoeff)\n",
    "\n",
    "fkb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read coefficients of subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.463498Z",
     "start_time": "2022-01-12T03:44:12.450431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read configcoeff.out\n",
    "configcoef = {}\n",
    "\n",
    "with open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/configmult.out','r') as fsubmult:\n",
    "    _ = next(fsubmult) #ignore first line\n",
    "    temp_submult = fsubmult.read() \n",
    "    temp_submult = pattern2.split(temp_submult) #split lines into blocks separated by 2 empty lines\n",
    "    \n",
    "for idx, submult in enumerate(temp_submult):\n",
    "    submult = submult.split('\\n') #split into number of subclusters \n",
    "    while(\"\" in submult) :\n",
    "        submult.remove(\"\") #remove empty blocks\n",
    "    configcoef[idx] = list(map(float,submult[1:])) #also ignore 1st line of each block\n",
    "configcoef.pop(list(configcoef.keys())[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read V-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.512581Z",
     "start_time": "2022-01-12T03:44:12.496884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read vmat.out\n",
    "vmat = {}\n",
    "with open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/vmat.out') as fvmat:\n",
    "    _ = next(fvmat) #ignore first lie\n",
    "    temp_vmat = fvmat.read()\n",
    "    temp_vmat = pattern2.split(temp_vmat) #split by 2 empty lines i.e. maxclusters\n",
    "    \n",
    "    while(\"\" in temp_vmat):\n",
    "        temp_vmat.remove(\"\") #remove empty blocks\n",
    "    \n",
    "    for clus_idx, mat in enumerate(temp_vmat):\n",
    "        mat = mat.split('\\n') #split by 1 empty line i.e. subclusters\n",
    "        mat_float = np.empty(list(map(int, mat[0].split(' '))))\n",
    "        for idx, row in enumerate(mat[1:]): #ignore first line\n",
    "            mat_float[idx] = list(map(float,row.split(' ')[:-1]))\n",
    "        \n",
    "        vmat[clus_idx] = mat_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Cluster Mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermult = {}\n",
    "\n",
    "with open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/clusmult.out', 'r') as fcm:\n",
    "    _ = next(fcm)  # Ignore first line\n",
    "    temp_mult = fcm.read()\n",
    "    temp_mult = temp_mult.split('\\n')  # split by line\n",
    "\n",
    "for idx, mult in enumerate(temp_mult):\n",
    "    if mult == '':\n",
    "        continue\n",
    "    clustermult[idx] = float(mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ECI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.545786Z",
     "start_time": "2022-01-12T03:44:12.538981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read eci\n",
    "eci = {}\n",
    "\n",
    "with open('BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/eci_tetrahedron.out') as feci:\n",
    "    _ = next(feci) #Ignore first line\n",
    "    temp_eci = feci.read()\n",
    "    temp_eci = temp_eci.split('\\n') #split by line\n",
    "\n",
    "for idx, eci_val in enumerate(temp_eci):\n",
    "    if eci_val == '':\n",
    "        continue\n",
    "    eci[idx] = float(eci_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.556998Z",
     "start_time": "2022-01-12T03:44:12.551521Z"
    }
   },
   "outputs": [],
   "source": [
    "#In this notebook ECI's are declared manually\n",
    "eci_2 = 0.01\n",
    "eci_3 = 2*eci_2\n",
    "eci = {0: 0, 1: 0.0, 2: 0.04, 3: 0, 4: 0, 5: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:12.585203Z",
     "start_time": "2022-01-12T03:44:12.573288Z"
    }
   },
   "outputs": [],
   "source": [
    "corrs1 = np.array([1., 1., 1., 1., 1., 1.]) #Pure B\n",
    "corrs0 = np.array([1., -1., 1., 1., -1., 1.]) #Pure A\n",
    "corrssqs = np.array([1.    , 0.0   , 0.25,0.25  , 0.125 , 0.0625]) # AB - sqs\n",
    "corrsrand = np.array([1.   , *np.random.uniform(-1, 1, 5)]) \n",
    "corrs_rnd = np.array([1.    , 0.0   , 0.0 , *np.random.uniform(-1, 1, 18)]) # AB - sqs\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_disordered = np.array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  0., 0., 0., 0., 0., 0., 0.,  0.])                              \n",
    "corrs_ordered = np.array([1., 0., 0., -0.3125, 0.6135, -0.5207, -0.3747, -0.577,  0.1245, 0.0003,  \n",
    "                          0.0358, -0.3749, 0.1439, 0.604, -0.3614,  \n",
    "                          1., -0.4336,  1., -0.333,  -0.1433, -1.]) \n",
    "corrs_100_nocons = np.array([1., 0.,     -0.,     -0.1701,  0.5731, -0.3322, -0.027,  -0.2143, -0.0945,\n",
    "                             0.155,  -0.1076, -0.2022,  0.1369,  0.3075, -0.4804,  0.3786, -0.205,\n",
    "                             0.3327, 0.0983, -0.169,  -0.9824])\n",
    "corrs_1000_nocons = np.array([1.,      0.,      0.,     -0.1366,  0.4484, -0.2673, -0.0892,  0.0189, -0.1465,\n",
    "                              0.1764, -0.0639, -0.0891,  0.0927,  0.2353, -0.5212,  0.4689, -0.2016,  0.2252,\n",
    "                              0.049,  -0.088,  -0.9813])\n",
    "corrs_100_cons = np.array([ 1.,      0.,     -0.,    -0.1362,  0.5376, -0.3492, -0.0616, -0.2397, -0.2259,\n",
    "                           0.1227, -0.1948, -0.2146,  0.1637,  0.3222, -0.4231,  0.1585, -0.1197,  0.1382,\n",
    "                           0.1078, -0.0774, -0.5255])\n",
    "corrs_1000_cons = np.array([1., 0., 0., -0.1383,  0.4167, -0.3007, -0.0989,  0.0098, -0.1904,\n",
    "                       0.1229, -0.1023, -0.0726,  0.0829,  0.2475, -0.474,  0.2463, -0.1156,  0.1298, \n",
    "                       0.0672, -0.048,  -0.7431])\n",
    "corrs_ordered_limited = np.array([ 1.,     0.,    -0.,     -0.4285,  0.2758, -0.4911,  0.5712, -0.4949,  0.0178,\n",
    "                                  -0.1429, -0.7139, -0.6248,  1.,      0.2455,  0.2781, -1.,      1.,     -1.,\n",
    "                                  0.9821, -0.6803,  1.    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_test = np.array([ 1.,     -0.,     -0.,     -0.2919,  0.5849, -0.3281,  0.0351, -0.2588, -0.0758,\n",
    "                       0.143,  -0.0185, -0.2273,  0.0202,  0.3161, -0.5061,  0.6908, -0.4132,  0.5505,\n",
    "                       0.051,  -0.0606, -0.9868])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.     ,  0.     , -0.     , -0.03906,  0.04059, -0.00781,\n",
       "        0.0625 ,  0.     ,  0.     ,  0.00586, -0.00338, -0.0293 ,\n",
       "       -0.02368, -0.00586, -0.00338,  0.02734, -0.0203 , -0.01172,\n",
       "        0.00781,  0.     , -0.00391], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs_rnd = subprocess.run(['/home/sayan/bin/corrdump', '-c', \n",
    "                       '-cf=BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/clusters.out', \n",
    "                       '-s=BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/ecifit_9/str.in', \n",
    "                       '-l=BCC_A2/sqs_lev=3_a_Mo=0.33333,a_V=0.33333,a_W=0.33333/lat.in'],\n",
    "                      stdout=subprocess.PIPE,\n",
    "                      stderr=subprocess.PIPE,\n",
    "                      check=True\n",
    "                     )\n",
    "corrs_rnd = corrs_rnd.stdout.decode('utf-8').split('\\t')[:-1]\n",
    "corrs_rnd = np.array(corrs_rnd, dtype=np.float32)  # convert to arrays\n",
    "corrs_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.        0.        0.        0.049671 -0.013826  0.064769  0.152303\n",
      " -0.023415 -0.023414  0.157921  0.076743 -0.046947  0.054256 -0.046342\n",
      " -0.046573  0.024196 -0.191328 -0.172492 -0.056229 -0.101283  0.031425]\n"
     ]
    }
   ],
   "source": [
    "def check_result_validity(corrs):\n",
    "    try:\n",
    "        for config_idx in configcoef:\n",
    "            assert np.isclose(np.inner(configcoef[config_idx], vmat[config_idx] @ corrs),\n",
    "                              1.0,\n",
    "                              rtol=1e-3,\n",
    "                              atol=1e-9\n",
    "                              )\n",
    "    except AssertionError:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "while True:\n",
    "    #corrs0 = np.array([1.    , 0.0   , 0.0 , *np.random.normal(0, 0.01, 18)])\n",
    "    corrs_rnd = np.array([1.    , 0.0   , 0.0 , *np.random.normal(0, 0.1, 18)])\n",
    "    if check_result_validity(corrs_rnd):\n",
    "        print(corrs_rnd)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_rnd = np.array([1.    , 0.0   , 0.0 , *np.random.normal(0, 0.1, 18)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disordered\n",
      "[1.]\n",
      "[0.333 0.333 0.333]\n",
      "[0.111 0.111 0.111 0.111 0.111 0.111]\n",
      "[0.111 0.111 0.111 0.111 0.111 0.111]\n",
      "[0.037 0.037 0.037 0.037 0.037 0.037 0.037 0.037 0.037 0.037 0.037 0.037\n",
      " 0.037 0.037 0.037 0.037 0.037 0.037]\n",
      "[0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012\n",
      " 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012]\n",
      "\n",
      "Ordered\n",
      "[1.]\n",
      "[0.333 0.333 0.333]\n",
      "[0.076 0.188 0.069 0.    0.146 0.118]\n",
      "[0.069 0.076 0.187 0.167 0.09  0.056]\n",
      "[ 0.     0.038  0.038  0.118  0.031  0.     0.069 -0.     0.118  0.\n",
      "  0.     0.028  0.     0.038  0.031  0.049  0.059  0.028]\n",
      "[ 0.     0.     0.     0.069  0.     0.     0.     0.038 -0.     0.\n",
      "  0.    -0.     0.049  0.031 -0.     0.     0.     0.     0.     0.028\n",
      " -0.   ]\n",
      "\n",
      "limitd\n",
      "[1.]\n",
      "[0.333 0.333 0.333]\n",
      "[0.063 0.161 0.108 0.032 0.14  0.085]\n",
      "[0.175 0.032 0.127 0.176 0.125 0.081]\n",
      "[0.032 0.032 0.    0.088 0.042 0.067 0.057 0.    0.104 0.    0.032 0.004\n",
      " 0.086 0.    0.023 0.088 0.052 0.01 ]\n",
      "[-0.     0.032  0.     0.     0.025  0.06  -0.    -0.    -0.     0.\n",
      "  0.     0.     0.088  0.016  0.006 -0.     0.     0.     0.032  0.004\n",
      " -0.   ]\n",
      "test\n",
      "[1.]\n",
      "[0.333 0.333 0.333]\n",
      "[0.079 0.184 0.071 0.019 0.13  0.132]\n",
      "[0.115 0.084 0.134 0.131 0.118 0.081]\n",
      "[0.011 0.045 0.023 0.096 0.043 0.006 0.089 0.012 0.083 0.001 0.007 0.04\n",
      " 0.015 0.028 0.028 0.033 0.069 0.035]\n",
      "[0.    0.011 0.    0.063 0.015 0.    0.011 0.022 0.    0.    0.005 0.\n",
      " 0.033 0.028 0.    0.001 0.    0.    0.007 0.034 0.   ]\n"
     ]
    }
   ],
   "source": [
    "print('Disordered')\n",
    "for vm in vmat.values():\n",
    "    #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "    print(vm @ corrs_disordered)\n",
    "print()\n",
    "print('Ordered')\n",
    "for vm in vmat.values():\n",
    "    #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "    print(vm @ corrs_ordered)\n",
    "print()\n",
    "print('limitd')\n",
    "for vm in vmat.values():\n",
    "    #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "    print(vm @ corrs_ordered_limited)\n",
    "# print('100 constrained')\n",
    "# for vm in vmat.values():\n",
    "#     #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "#     print(vm @ corrs_100_cons)\n",
    "# print()\n",
    "# print('100 not constrained')\n",
    "# for vm in vmat.values():\n",
    "#     #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "#     print(vm @ corrs_100_nocons)\n",
    "# print()\n",
    "# print('1000 not constrained')\n",
    "# for vm in vmat.values():\n",
    "#     #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "#     print(vm @ corrs_1000_nocons)\n",
    "# print()\n",
    "# print('1000 constrained')\n",
    "# for vm in vmat.values():\n",
    "#     #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "#     print(vm @ corrs_1000_cons)\n",
    "# print()\n",
    "print('test')\n",
    "for vm in vmat.values():\n",
    "    #print(f\"{' '.join(map(str, vm @ corrs_ordered))}\")\n",
    "    print(vm @ corrs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1.0],\n",
       " 1: [1.0, 1.0, 1.0],\n",
       " 2: [1.0, 2.0, 2.0, 1.0, 2.0, 1.0],\n",
       " 3: [1.0, 2.0, 2.0, 1.0, 2.0, 1.0],\n",
       " 4: [1.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0],\n",
       " 5: [1.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  8.0,\n",
       "  4.0,\n",
       "  8.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  8.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Log Absolute functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_arr = np.array(list(clustermult.values()))\n",
    "eci_arr = np.array(list(eci.values()))\n",
    "\n",
    "all_vmat = np.vstack([vmat for vmat in vmat.values()])\n",
    "mults_config = np.asarray(list(itertools.chain.from_iterable(list(configcoef.values()))))\n",
    "mults_eci = np.multiply(mult_arr,eci_arr)\n",
    "all_kb = np.array(list(itertools.chain.from_iterable([[kb for _ in range(len(configcoef[idx]))] for idx, kb in kb.items()])))\n",
    "\n",
    "multconfig_kb = np.multiply(mults_config,all_kb)\n",
    "\n",
    "rhologrho = lambda rho: rho * np.log(np.abs(rho))\n",
    "vrhologrho = np.vectorize(rhologrho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:14.053678Z",
     "start_time": "2022-01-12T03:44:14.037882Z"
    }
   },
   "outputs": [],
   "source": [
    "def F(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    mu - chemical potentials\n",
    "    \n",
    "    Output:\n",
    "    F = H + kB*T*SUM(rho * log(rho))\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_corrsum(vmat,corrs):\n",
    "        assert len(vmat) == len(corrs)\n",
    "        corrsum = np.inner(vmat,corrs)\n",
    "        if corrsum == 0:\n",
    "            return 0\n",
    "            #corrsum = np.finfo(float).tiny\n",
    "\n",
    "        return corrsum * np.log(np.abs(corrsum))\n",
    "    \n",
    "    def per_cluster_sum(corrs,vmat,configcoef):\n",
    "        config_sum = np.sum([coef * get_corrsum(vmat[config_idx],corrs) for config_idx, coef in enumerate(configcoef)\n",
    "                            ])\n",
    "                      \n",
    "        return config_sum\n",
    "    \n",
    "    H = np.sum([cluster['mult']*eci[cluster_idx]*corrs[cluster_idx] \n",
    "                for cluster_idx, cluster in clusters.items()\n",
    "               ])\n",
    "    \n",
    "    S = np.sum([kb[cluster_idx]*per_cluster_sum(corrs,\n",
    "                                                vmat[cluster_idx],\n",
    "                                                configcoef[cluster_idx],)\n",
    "                for cluster_idx in configs.keys()\n",
    "               ])\n",
    "    \n",
    "    #rho1 = np.matmul(vmat[1],corrs) #mole fractions\n",
    "    \n",
    "    return H + kB*T*S #- np.sum(mu*rho1)\n",
    "\n",
    "def F_optim_np(corrs, vmat, kb, clusters,clustermult, configs, configcoef,T,eci):\n",
    "\n",
    "    H = mults_eci @ corrs\n",
    "    #S = np.inner(multconfig_kb,vrhologrho(all_vmat @ corrs))\n",
    "    S = multconfig_kb @ vrhologrho(all_vmat @ corrs)\n",
    "    \n",
    "    return H + kB*T*S #- np.sum(mu*rho1)\n",
    "    #return jnp.dot(mults_eci,corrs) + kB*T*jnp.vdot(multconfig_kb,vrhologrho(jnp.dot(all_vmat, corrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T03:44:14.202761Z",
     "start_time": "2022-01-12T03:44:14.183884Z"
    }
   },
   "outputs": [],
   "source": [
    "#f0 = F(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#f1 = F(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#fsqs = F(corrssqs, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "fnew = F_optim_np(corrs_rnd, vmat, kb, clusters, clustermult, configs, configcoef,100,eci)\n",
    "#frand = F(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "frnd = F(corrs_rnd, vmat, kb, clusters, configs, configcoef,100,eci)\n",
    "#print(f\"Corrs 0: {f0:.2f} -- Corrs 1: {f1:.2f} -- Corrs Rand: {frand:.2f} -- Corrs SQS: {fsqs:.2f} -- Corrs New: {fnew:.2f} -- Corrs RND: {frnd:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "f_disordered_100 = F(corrs_disordered, vmat, kb, clusters, configs, configcoef,100,eci)\n",
    "f_ordered_100 = F(corrs_ordered, vmat, kb, clusters, configs, configcoef,0,eci)\n",
    "f_disordered_1000 = F(corrs_disordered, vmat, kb, clusters, configs, configcoef,1000,eci)\n",
    "f_ordered_1000 = F(corrs_ordered, vmat, kb, clusters, configs, configcoef,1000,eci)\n",
    "f_cons_100 = F(corrs_100_cons, vmat, kb, clusters, configs, configcoef,100,eci)\n",
    "f_nocons_100 = F(corrs_100_nocons, vmat, kb, clusters, configs, configcoef,100,eci)\n",
    "f_cons_1000 = F(corrs_1000_cons, vmat, kb, clusters, configs, configcoef,1000,eci)\n",
    "f_nocons_1000 = F(corrs_1000_nocons, vmat, kb, clusters, configs, configcoef,1000,eci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.282847802635997"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2d = np.linalg.norm((corrs_disordered - corrs_ordered),ord=2)\n",
    "o2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.26712670480163947, -2.1103008009948354e-06)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2d/2 - np.linalg.norm(-1*(corrs_disordered - corrs_1000_nocons)), o2d/2 - np.linalg.norm(-1*(corrs_disordered - corrs_1000_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5597072146604241, -0.5626475872046909)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_cons_1000, f_nocons_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.455926768380781, -0.23778639890000003)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_disordered_1000, f_ordered_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set up F Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T02:31:49.101174Z",
     "start_time": "2022-01-12T02:31:49.072407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def F_jacobian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    mu - chemical potentials\n",
    "    \n",
    "    Output:\n",
    "    Vector representation gradient of F with Corrs\n",
    "    [dF/dcorr0, dF/dcorr1, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_kth_elem_jac(corrs, vmat, kb, clusters, configs, configcoef,corr_idx):\n",
    "        \n",
    "        dS_k = np.sum([kb[cluster_idx]*per_cluster_sum_jac(corrs,\n",
    "                                                           vmat[cluster_idx],\n",
    "                                                           configcoef[cluster_idx],\n",
    "                                                           corr_idx,) \n",
    "                       for cluster_idx in configs.keys()\n",
    "                      ])\n",
    "        return dS_k\n",
    "    \n",
    "    def get_corrsum_jac(vmat,corrs):\n",
    "        assert len(vmat) == len(corrs)\n",
    "        corrsum = np.inner(vmat,corrs)\n",
    "        if corrsum == 0:\n",
    "            return np.NINF\n",
    "            #corrsum = np.finfo(float).tiny\n",
    "\n",
    "        return 1 + math.log(np.abs(corrsum))\n",
    "\n",
    "    def per_cluster_sum_jac(corrs,vmat,configcoef,corr_idx):\n",
    "        \n",
    "        config_sum = np.sum([coef * vmat[config_idx][corr_idx] * get_corrsum_jac(vmat[config_idx],corrs) for config_idx, coef in enumerate(configcoef)\n",
    "                            ])\n",
    "\n",
    "        \n",
    "        return config_sum\n",
    "    \n",
    "    #def get_dmu(vmat, mu, corr_idx):   \n",
    "    #    return np.sum([mu[i]*vmat[1][i][corr_idx] for i, _ in enumerate(configcoef[1])])\n",
    "\n",
    "    #dmu = np.array([get_dmu(vmat, mu, corr_idx) for corr_idx, _ in enumerate(corrs)])\n",
    "    \n",
    "    dH = np.array([cluster['mult']*eci[cluster_idx] for cluster_idx, cluster in clusters.items()])\n",
    "    \n",
    "    dS = np.array([get_kth_elem_jac(corrs, vmat, kb, clusters, configs, configcoef,corr_idx) for corr_idx, _ in enumerate(corrs)])\n",
    "    \n",
    "    return dH + kB*T*dS #- dmu\n",
    "\n",
    "def F_jacobian_optim(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "\n",
    "    dH = mults_eci \n",
    "    dS = all_vmat.T @ (multconfig_kb * (1 + np.log(np.abs(all_vmat @ corrs))))\n",
    "    \n",
    "    return dH + kB*T*dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T02:33:10.816432Z",
     "start_time": "2022-01-12T02:33:10.782383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#f_jaco0 = F_jacobian(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#f_jaco1 = F_jacobian(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#f_jacosqs = F_jacobian(corrssqs, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#f_jacorand = F_jacobian(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#f_jacornd = F_jacobian(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "frnd_jac = F_jacobian(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#frand = F(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "fnew_jac = F_jacobian_optim(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#print(f\"Corrs 0: {f_jaco0} \\nCorrs 1: {f_jaco1} \\nCorrs Rand {f_jacorand} \\nCorrs SQS {f_jacosqs} \\nCorrs RND {f_jacornd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-11.088305,   0.000582,  -0.010316,   0.125114,  -0.04093 ,\n",
       "          0.036542,   0.046397,   0.065335,  -0.040581,  -0.001179,\n",
       "          0.000538,  -0.000283,   0.000173,   0.004662,   0.000425,\n",
       "         -0.000313,  -0.001683,  -0.001697,  -0.001062,  -0.002441,\n",
       "         -0.000056]),\n",
       " array([-11.088305,   0.000582,  -0.010316,   0.125114,  -0.04093 ,\n",
       "          0.036542,   0.046397,   0.065335,  -0.040581,  -0.001179,\n",
       "          0.000538,  -0.000283,   0.000173,   0.004662,   0.000425,\n",
       "         -0.000313,  -0.001683,  -0.001697,  -0.001062,  -0.002441,\n",
       "         -0.000056]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frnd_jac, fnew_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set up Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T02:33:31.751071Z",
     "start_time": "2022-01-12T02:33:31.730113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def F_hessian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    Vector representation gradient of F with Corrs\n",
    "    [[d^2F/dcorr0 dcorr0, d^2F/dcorr0 dcorr1, ..., d^2F/dcorr0 dcorrn],\n",
    "     [d^2F/dcorr1 dcorr0, d^2F/dcorr1 dcorr1, ..., d^2F/dcorr1 dcorrn],\n",
    "     .\n",
    "     .\n",
    "     .\n",
    "     [d^2F/dcorrn dcorr0, d^2F/dcorrn dcorr1, ..., d^2F/dcorrn dcorrn],\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_corrsum_hess(vmat,corrs):\n",
    "        assert len(vmat) == len(corrs)\n",
    "        \n",
    "        corrsum = np.inner(vmat,corrs)\n",
    "        if corrsum == 0:\n",
    "            return 1/np.PINF\n",
    "            #corrsum = np.finfo(float).tiny\n",
    "\n",
    "        return corrsum\n",
    "    \n",
    "    def get_config_val(corrs,vmat,configcoef,corr_idx_1,corr_idx_2):\n",
    "        \n",
    "        config_val = np.sum([coef * vmat[config_idx][corr_idx_1] * vmat[config_idx][corr_idx_2] / get_corrsum_hess(vmat[config_idx],corrs) \n",
    "                             for config_idx, coef in enumerate(configcoef)\n",
    "                            ])\n",
    "        \n",
    "        return config_val\n",
    "    \n",
    "    def get_hessian_elem(corrs, vmat, kb, clusters, configs, configcoef,T,eci,corr_idx_1,corr_idx_2):\n",
    "        \n",
    "        hess_elem = np.sum([kb[cluster_idx] * get_config_val(corrs,\n",
    "                                                             vmat[cluster_idx],\n",
    "                                                             configcoef[cluster_idx],\n",
    "                                                             corr_idx_1,\n",
    "                                                             corr_idx_2\n",
    "                                                            ) \n",
    "                            for cluster_idx in configs.keys()\n",
    "                           ])\n",
    "        return hess_elem\n",
    "    \n",
    "    d2F = np.empty([len(corrs),len(corrs)])\n",
    "    \n",
    "    d2F = np.array([[get_hessian_elem(corrs, vmat, kb, clusters, configs, configcoef, T, eci, corr_idx_1, corr_idx_2) for corr_idx_2, _ in enumerate(corrs)] for corr_idx_1, _ in enumerate(corrs)])\n",
    "    \n",
    "    return kB*T*d2F\n",
    "\n",
    "def F_hessian_optim(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \n",
    "    t_0 = all_vmat @ corrs\n",
    "    #d2S = ((multconfig_kb / t_0)[:, np.newaxis] * all_vmat).T @ all_vmat\n",
    "    d2S = (np.diag(multconfig_kb / (all_vmat @ corrs)).T @ all_vmat).T @ all_vmat\n",
    "    \n",
    "    return kB*T*d2S\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frnd_hess = F_hessian(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#frand = F(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "fnew_hess = F_hessian_optim(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 ms ± 470 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "F_hessian(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3 µs ± 244 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "F_hessian_optim(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(frnd_hess, fnew_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "H = hessian(F_optim_np)(corrs_rnd, vmat, kb, clusters, clustermult, configs, configcoef,T,eci)\n",
    "print(\"hessian, with shape\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H.to_py()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T02:34:23.378961Z",
     "start_time": "2022-01-12T02:34:23.316529Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_hess0 = F_hessian(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hess1 = F_hessian(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hessrand = F_hessian(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hesssqs = F_hessian(corrssqs, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hessrnd = F_hessian(corrs_rnd, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "#print(f\"Corrs 0:\\n {f_hess0} \\n Corrs 1:\\n {f_hess1}\\n Corrs Rand:\\n {f_hessrand} \\n Corrs SQS:\\n {f_hesssqs} \\n RND SQS:\\n {f_hessrnd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T02:34:59.127673Z",
     "start_time": "2022-01-12T02:34:59.101144Z"
    },
    "code_folding": [
     43,
     58
    ]
   },
   "outputs": [],
   "source": [
    "#NOT BEING USED ANY MORE SINCE IT IS TRIVIALLY TRUE\n",
    "def constraint_rhos_sum(corrs, vmat, clusters, configcoef,):\n",
    "    \"\"\"\n",
    "    Constraints the sum of each rho. As of now, it's done in a weird way, where the total sum of the array:\n",
    "    [1 - sum(rho), .... ] is constrained to sum to 0. This along with the constraint that each rho is between\n",
    "    0 and 1, seems to make it work. I think that by the this might be a redundant constraint as well.\n",
    "    \"\"\"\n",
    "    rho_sum = []\n",
    "\n",
    "    def clus_prob(cluster_idx):\n",
    "        rho = np.matmul(vmat[cluster_idx],corrs)\n",
    "        return rho\n",
    "    \n",
    "    for cluster_idx, _ in clusters.items():\n",
    "        rho = clus_prob(cluster_idx)\n",
    "        rho_sum.append(np.sum(configcoef[cluster_idx]*rho))\n",
    "    \n",
    "    return np.sum(1 - np.array(rho_sum))\n",
    "\n",
    "def constraint_singlet(corrs,FIXED_CORR_1):\n",
    "    \"\"\"\n",
    "    constrains the 1-pt correlation:\n",
    "    corrs[1] = FIXED_CORR_1\n",
    "    \"\"\"\n",
    "    return corrs[1] - FIXED_CORR_1   \n",
    "\n",
    "def constraint_zero(corrs):\n",
    "    \"\"\"\n",
    "    constrains the 1-pt correlation:\n",
    "    corrs[0] = 1\n",
    "    \"\"\"\n",
    "    return 1 - corrs[0]\n",
    "\n",
    "def constraint_hessian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    hessian_F = hessian(F)\n",
    "    return np.amin(np.real(eigvals(hessian_F(corrs,vmat, kb, clusters, configs, configcoef,T,eci))))\n",
    "\n",
    "def constraint_NN(corrs,FIXED_CORR_2):\n",
    "    \"\"\"\n",
    "    constrains the 2-pt correlation:\n",
    "    corrs[2] = FIXED_CORR_2\n",
    "    \"\"\"\n",
    "    return corrs[2] - FIXED_CORR_2 \n",
    "\n",
    "class MyBounds:\n",
    "    \"\"\"\n",
    "    Class to constrain the trial correlations of Basin Hopping\n",
    "    \"\"\"\n",
    "    def __init__(self,xmax=[1]*6, xmin=[-1]*6,):\n",
    "        self.xmax = np.array(xmax)\n",
    "        self.xmin = np.array(xmin)\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        x = kwargs[\"x_new\"]\n",
    "        tmax = bool(np.all(x <= self.xmax))\n",
    "        tmin = bool(np.all(x >= self.xmin))\n",
    "\n",
    "        return tmax and tmin\n",
    "    \n",
    "class MyTakeStep:\n",
    "    \n",
    "    def __init__(self, vmat, clusters, stepsize=0.1):\n",
    "        self.stepsize = stepsize\n",
    "        self.vmat = vmat\n",
    "        self.clusters = clusters\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        s = self.stepsize\n",
    "        \n",
    "        validcorr = np.ones(len(self.clusters), dtype=bool)\n",
    "        \n",
    "        for _ in iter(int,1):\n",
    "            x_trial = x + self.rng.uniform(-s, s, x.shape)\n",
    "            for cluster_idx, _ in self.clusters.items():\n",
    "                rho = np.matmul(self.vmat[cluster_idx],x_trial)\n",
    "                validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "            if bool(np.all(validcorr)):\n",
    "                break\n",
    "            \n",
    "        return x_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initial Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T22:40:43.258307Z",
     "start_time": "2022-01-11T22:40:43.247670Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6 is different from 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [336]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m corrs0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, corr1, corr2, corr3, corr4, corr5])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_idx, _ \u001b[38;5;129;01min\u001b[39;00m clusters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 20\u001b[0m     rho \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvmat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcluster_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcorrs0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     validcorr[cluster_idx] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mall(rho \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(np\u001b[38;5;241m.\u001b[39mall(validcorr)):\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6 is different from 21)"
     ]
    }
   ],
   "source": [
    "FIXED_CORR1 = 0.0\n",
    "            \n",
    "for c in iter(int,1): #infinite loop till a valid starting correlations are found\n",
    "    corr0 = 1.0\n",
    "    corr1 = FIXED_CORR1\n",
    "    validcorr = np.ones(len(clusters), dtype=bool)\n",
    "    if corr1 <=0 :\n",
    "        corr2 = np.random.uniform(-2*corr1 - 1, 1)\n",
    "        corr3 = np.random.uniform(-2*corr1 - 1, 1)\n",
    "        #corr4 = np.random.uniform(-corr1+corr3-1, 1)\n",
    "    else:\n",
    "        corr2 = np.random.uniform(2*corr1 - 1, 1)\n",
    "        corr3 = np.random.uniform(2*corr1 - 1, 1)\n",
    "        #corr4 = np.random.uniform(corr1+corr3-1, 1)\n",
    "\n",
    "    corr4 = np.random.uniform(corr1+corr2-1,corr1-corr2+1)\n",
    "    corr5 = np.random.uniform(2*corr3-1,1)\n",
    "    corrs0 = np.array([1, corr1, corr2, corr3, corr4, corr5])\n",
    "    for cluster_idx, _ in clusters.items():\n",
    "        rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "        validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "    \n",
    "    if bool(np.all(validcorr)):\n",
    "        print(corrs0)\n",
    "        print(c)\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T22:40:44.396147Z",
     "start_time": "2022-01-11T22:40:44.370132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_valid_corrs(FIXED_CORR1,FIXED_CORR2=None):\n",
    "    \n",
    "    for _ in iter(int,1):\n",
    "        corr0 = 1.0\n",
    "        corr1 = FIXED_CORR1\n",
    "        validcorr = np.ones(len(clusters), dtype=bool)\n",
    "        if corr1 <=0 :\n",
    "            if FIXED_CORR2 is None:\n",
    "                corr2 = np.random.uniform(-2*corr1 - 1, 1)\n",
    "            else:\n",
    "                corr2 = FIXED_CORR2\n",
    "            corr3 = np.random.uniform(-2*corr1 - 1, 1)\n",
    "        else:\n",
    "            if FIXED_CORR2 is None:\n",
    "                corr2 = corr2 = np.random.uniform(2*corr1 - 1, 1)\n",
    "            else:\n",
    "                corr2 = FIXED_CORR2\n",
    "            corr3 = np.random.uniform(2*corr1 - 1, 1)\n",
    "\n",
    "        corr4 = np.random.uniform(corr1+corr2-1,corr1-corr2+1)\n",
    "        corr5 = np.random.uniform(2*corr3-1,-2*corr1 + 2*corr4 + 1)\n",
    "        corrs0 = np.array([1, corr1, corr2, corr3, corr4, corr5])\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "            validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "        \n",
    "        if bool(np.all(validcorr)):\n",
    "            print(corrs0)\n",
    "            break \n",
    "    return corrs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T01:29:47.247899Z",
     "start_time": "2021-11-16T01:29:45.641Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.        0.        0.        0.002576 -0.000744 -0.019188 -0.000265\n",
      "  0.000602  0.024632 -0.001924  0.003015 -0.000347 -0.011687  0.011428\n",
      "  0.007519  0.00791  -0.009094  0.014028 -0.014019  0.005869  0.021905]\n"
     ]
    }
   ],
   "source": [
    "#corrs0 = get_valid_corrs(np.random.uniform(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T22:40:49.845552Z",
     "start_time": "2022-01-11T22:40:49.832117Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.33333 0.33333 0.33333\n",
      "0.11139616423913207 0.11089526569817265 0.11103854430765622 0.10965427474957588 0.11278045955225148 0.10951099614009231\n",
      "0.11108054043298837 0.11118269064022583 0.11106677157817334 0.11309729456670795 0.10905001479306622 0.11321321362876044\n",
      "0.037292197336296885 0.036955686665568255 0.03715828023726691 0.03768354984532849 0.03626602918727589 0.037624234883113394 0.037226312204636784 0.03606859709523387 0.03761035639830199 0.036900090820371614 0.03669539495684884 0.03848490007422216 0.03657200248562808 0.03816841290244469 0.036308128919583425 0.03852389420219808 0.036098344324730226 0.03711433101865711\n",
      "0.012663312396959224 0.012273265638644427 0.012365726810314559 0.013032586103279257 0.011930470340627138 0.012286048948207947 0.011799631954419063 0.012892947394527385 0.012039448629734656 0.012239637233776959 0.01304582468444182 0.011909701201837954 0.012621617726319147 0.012106048358589595 0.012302512127894677 0.012416626355868168 0.012454099655047119 0.013458347354179665 0.012011698265956288 0.011990652787299019 0.012831177568231295\n"
     ]
    }
   ],
   "source": [
    "for vm in vmat.values():\n",
    "    print(f\"{' '.join(map(str, vm @ corrs0))}\")\n",
    "corrs_rnd=corrs0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T01:30:11.542692Z",
     "start_time": "2021-11-16T01:30:11.534874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.  , -0.67, -0.33,  0.  ,  0.33,  0.67,  1.  ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(-1, 1, num=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T01:42:33.790174Z",
     "start_time": "2022-01-12T01:42:33.777375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0.0, 2: 0.04, 3: 0, 4: 0, 5: 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.  , -0.5 ,  0.59,  0.01,  0.15, -0.01])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T01:57:21.203040Z",
     "start_time": "2022-01-12T01:57:04.713764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5841c8fd645543b9907c6934424eb30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 521, function evaluations: 8138, CG iterations: 509, optimality: 1.70e-05, constraint violation: 0.00e+00, execution time:  5.7 s.\n",
      "Found new minimum for x:-0.5, T:100 fun: -0.017157678971778927\n",
      "Current minimum correlations: [ 1.   -0.5   0.    0.38  0.12 -0.24]\n",
      "[1.]\n",
      "[0.75 0.25]\n",
      "[0.5  0.25 0.  ]\n",
      "[0.6  0.15 0.1 ]\n",
      "[0.35 0.15 0.1  0.25 0.   0.  ]\n",
      "[0.19 0.15 0.1  0.   0.   0.  ]\n",
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 351, function evaluations: 5421, CG iterations: 339, optimality: 1.93e-05, constraint violation: 0.00e+00, execution time:  3.6 s.\n",
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 407, function evaluations: 6981, CG iterations: 395, optimality: 2.32e-05, constraint violation: 0.00e+00, execution time:  4.3 s.\n",
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 374, function evaluations: 5785, CG iterations: 362, optimality: 1.18e-06, constraint violation: 0.00e+00, execution time:  3.7 s.\n",
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 519, function evaluations: 8151, CG iterations: 507, optimality: 6.99e-07, constraint violation: 0.00e+00, execution time:  6.1 s.\n",
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 436, function evaluations: 6773, CG iterations: 424, optimality: 3.91e-06, constraint violation: 0.00e+00, execution time:  4.2 s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m jitter \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.001\u001b[39m, corrs0[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mshape)])\n\u001b[1;32m     41\u001b[0m corrstrial \u001b[38;5;241m=\u001b[39m corrs0\u001b[38;5;241m+\u001b[39mjitter  \n\u001b[0;32m---> 42\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcorrstrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrust-constr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m               \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigcoef\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43meci\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m               \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3-point\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m               \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBFGS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m               \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlinear_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraint_singlet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mFIXED_CORR_1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhess\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraint_zero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds_corrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mfun \u001b[38;5;241m<\u001b[39m MIN_RES_VAL:\n\u001b[1;32m     57\u001b[0m     MIN_RES \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_minimize.py:634\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[1;32m    632\u001b[0m                            constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_minimize_trustregion_constr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdogleg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_dogleg(fun, x0, args, jac, hess,\n\u001b[1;32m    639\u001b[0m                             callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.py:509\u001b[0m, in \u001b[0;36m_minimize_trustregion_constr\u001b[0;34m(fun, x0, args, grad, hess, hessp, bounds, constraints, xtol, gtol, barrier_tol, sparse_jacobian, callback, maxiter, verbose, finite_diff_rel_step, initial_constr_penalty, initial_tr_radius, initial_barrier_parameter, initial_barrier_tolerance, factorization_method, disp)\u001b[0m\n\u001b[1;32m    500\u001b[0m     _, result \u001b[38;5;241m=\u001b[39m equality_constrained_sqp(\n\u001b[1;32m    501\u001b[0m         fun_and_constr, grad_and_jac, lagrangian_hess,\n\u001b[1;32m    502\u001b[0m         x0, objective\u001b[38;5;241m.\u001b[39mf, objective\u001b[38;5;241m.\u001b[39mg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m         initial_constr_penalty, initial_tr_radius,\n\u001b[1;32m    506\u001b[0m         factorization_method)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr_interior_point\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     _, result \u001b[38;5;241m=\u001b[39m \u001b[43mtr_interior_point\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlagrangian_hess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_ineq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_ineq0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ_ineq0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_eq0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ_eq0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcanonical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_feasible\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_barrier_parameter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_barrier_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_constr_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_tr_radius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfactorization_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# Status 3 occurs when the callback function requests termination,\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# this is assumed to not be a success.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m result\u001b[38;5;241m.\u001b[39msuccess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/tr_interior_point.py:321\u001b[0m, in \u001b[0;36mtr_interior_point\u001b[0;34m(fun, grad, lagr_hess, n_vars, n_ineq, n_eq, constr, jac, x0, fun0, grad0, constr_ineq0, jac_ineq0, constr_eq0, jac_eq0, stop_criteria, enforce_feasibility, xtol, state, initial_barrier_parameter, initial_tolerance, initial_penalty, initial_trust_radius, factorization_method)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# Solves a sequence of barrier problems\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# Solve SQP subproblem\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     z, state \u001b[38;5;241m=\u001b[39m \u001b[43mequality_constrained_sqp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubprob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_and_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubprob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_and_jacobian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubprob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlagrangian_hessian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun0_subprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad0_subprob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstr0_subprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac0_subprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubprob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_radius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfactorization_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_lb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_ub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubprob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subprob\u001b[38;5;241m.\u001b[39mterminate:\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/equality_constrained_sqp.py:203\u001b[0m, in \u001b[0;36mequality_constrained_sqp\u001b[0;34m(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb, trust_ub, scaling)\u001b[0m\n\u001b[1;32m    201\u001b[0m S \u001b[38;5;241m=\u001b[39m scaling(x)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Get projections\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m Z, LS, Y \u001b[38;5;241m=\u001b[39m \u001b[43mprojections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactorization_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Compute least-square lagrange multipliers\u001b[39;00m\n\u001b[1;32m    205\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mLS\u001b[38;5;241m.\u001b[39mdot(c)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/projections.py:396\u001b[0m, in \u001b[0;36mprojections\u001b[0;34m(A, method, orth_tol, max_refin, tol)\u001b[0m\n\u001b[1;32m    392\u001b[0m     null_space, least_squares, row_space \\\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;241m=\u001b[39m augmented_system_projections(A, m, n, orth_tol, max_refin, tol)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQRFactorization\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    395\u001b[0m     null_space, least_squares, row_space \\\n\u001b[0;32m--> 396\u001b[0m         \u001b[38;5;241m=\u001b[39m \u001b[43mqr_factorization_projections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morth_tol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_refin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVDFactorization\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    398\u001b[0m     null_space, least_squares, row_space \\\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m=\u001b[39m svd_factorization_projections(A, m, n, orth_tol, max_refin, tol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/projections.py:183\u001b[0m, in \u001b[0;36mqr_factorization_projections\u001b[0;34m(A, m, n, orth_tol, max_refin, tol)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(R[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], np\u001b[38;5;241m.\u001b[39minf) \u001b[38;5;241m<\u001b[39m tol:\n\u001b[1;32m    181\u001b[0m     warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSingular Jacobian matrix. Using SVD decomposition to \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    182\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperform the factorizations.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msvd_factorization_projections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43morth_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmax_refin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# z = x - A.T inv(A A.T) A x\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnull_space\u001b[39m(x):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# v = P inv(R) Q.T x\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/projections.py:239\u001b[0m, in \u001b[0;36msvd_factorization_projections\u001b[0;34m(A, m, n, orth_tol, max_refin, tol)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"Return linear operators for matrix A using ``SVDFactorization`` approach.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# SVD Factorization\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m U, s, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Remove dimensions related with very small singular values\u001b[39;00m\n\u001b[1;32m    242\u001b[0m U \u001b[38;5;241m=\u001b[39m U[:, s \u001b[38;5;241m>\u001b[39m tol]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/linalg/decomp_svd.py:127\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#results_uniform = pd.DataFrame(columns = ['T', '1-point_corr', 'F','corrs'])\n",
    "NUM_TRIALS = 50\n",
    "MAX_TEMP = 1\n",
    "\n",
    "for temp in [100]:#tqdm(np.linspace(0, MAX_TEMP, num=11)):\n",
    "    for x in [-0.5]:#tqdm(np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,9)):\n",
    "        FIXED_CORR_1 = x\n",
    "        \n",
    "        MIN_RES_VAL = 1e5 #random large number\n",
    "        rho_pair = np.array([None,None])\n",
    "        \n",
    "        MIN_RES = corrs0\n",
    "\n",
    "        linear_constraints = []\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            \n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1,*[-1]*(len(clusters)-2)],\n",
    "                              [1, FIXED_CORR_1,*[1]*(len(clusters)-2)]\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 1,\n",
    "                   'maxiter' : 1000,\n",
    "                   'xtol'    : 1e-18,\n",
    "                   'gtol'    : 1e-18,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        #corrs0 = np.array([1, x, *np.random.uniform(-1, 1, 4)])\n",
    "        corrs0 = np.array([ 1.,     -0.5,     0.25,    0.25,   -0.125,   0.0625])\n",
    "        for _ in tqdm(range(NUM_TRIALS)):\n",
    "            jitter = np.array([0, 0, *np.random.normal(0, .001, corrs0[2:].shape)])\n",
    "            corrstrial = corrs0+jitter  \n",
    "            res = minimize(F,\n",
    "                           corrstrial,\n",
    "                           method='trust-constr',\n",
    "                           args=(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                           options=options,\n",
    "                           jac='3-point',\n",
    "                           hess=BFGS(),\n",
    "                           constraints=[*linear_constraints, \n",
    "                                        {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1],'hess':0},\n",
    "                                        {'fun': constraint_zero, 'type':'eq',},\n",
    "                                       ],\n",
    "                           bounds=bounds_corrs,\n",
    "                          )\n",
    "            \n",
    "            if res.fun < MIN_RES_VAL:\n",
    "                MIN_RES = res\n",
    "                MIN_RES_VAL = res.fun\n",
    "                print(f\"Found new minimum for x:{x}, T:{temp} fun: {MIN_RES_VAL}\")\n",
    "                print(f'Current minimum correlations: {res.x}')\n",
    "                for cluster_idx in clusters.keys():\n",
    "                    print(np.matmul(vmat[cluster_idx],res.x))\n",
    "        \n",
    "            \n",
    "        for cluster_idx in clusters.keys():\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "        \n",
    "        results_uniform = results_uniform.append({'T' : temp, \n",
    "                                           '1-point_corr' : x, \n",
    "                                           'F' : MIN_RES.fun, \n",
    "                                           'corrs': MIN_RES.x,\n",
    "                                          }, \n",
    "                                          ignore_index = True\n",
    "                                         )\n",
    "        \n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H:%M\")\n",
    "#results_uniform.to_pickle(f'results/uni_{eci[2]},{eci[3]}_{MAX_TEMP}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0.0, 2: 0.04, 3: 0, 4: 0, 5: 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Phase Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T02:20:38.989213Z",
     "start_time": "2021-11-11T02:20:38.935099Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for T in results_uniform['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_uniform[results_uniform['T'] == T]['1-point_corr'],\n",
    "                             y = results_uniform[results_uniform['T'] == T]['F'],\n",
    "                             mode='markers+lines',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F vs 1-point Corr\",\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"F\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Pair Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T02:20:43.258139Z",
     "start_time": "2021-11-11T02:20:43.199726Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = results_uniform[results_uniform['1-point_corr'] == 0.0]['T'],\n",
    "                         y = results_uniform[results_uniform['1-point_corr'] == 0.0]['corrs'].str[2],\n",
    "                         mode='markers+lines',\n",
    "                        )\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"T vs 2-point Corr\",\n",
    "    xaxis_title=\"T\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T18:58:05.529681Z",
     "start_time": "2021-09-12T18:58:05.511796Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Basin Hopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Defining the Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def basin_hopping_callback(corrs, G, accept):\n",
    "    \"\"\"\n",
    "    Function to print diagnostic while fitting. \n",
    "    Not being used right now\n",
    "    \"\"\"\n",
    "    if accept == True:\n",
    "        clear_output(wait=True)\n",
    "        print(f'1-pt Correlation: {corrs[1]:.2f}')\n",
    "        print(f'Concentation: {(corrs[1] - (-1))/(1 - (-1)):.2f}')\n",
    "        print(f'New Minima found --> G: {G:.2f}')\n",
    "        print('Current Rho:')\n",
    "        for cluster_idx in clusters:\n",
    "            print(np.matmul(vmat[cluster_idx],corrs))\n",
    "        print(\"===========================\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Optimisation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T18:11:15.585712Z",
     "start_time": "2021-09-16T17:29:34.718589Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_basinhopping = pd.DataFrame(columns = ['T', '1-point_corr', 'F','corrs'])\n",
    "MAX_TEMP = 500\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H-%M\")\n",
    "\n",
    "for temp in tqdm(np.linspace(0, MAX_TEMP, num=21)):\n",
    "    for x in [0]:#tqdm(np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,9)):\n",
    "\n",
    "        FIXED_CORR_1 = x\n",
    "\n",
    "        rho_pair = np.array([None,None])\n",
    "        \n",
    "        #Make a first guess\n",
    "        for _ in iter(int,1): #infinite loop till a valid starting correlations are found\n",
    "            corrs0 = np.array([1, x, *np.random.uniform(-1, 1, 4)])\n",
    "            validcorr = np.ones(len(clusters), dtype=bool)\n",
    "\n",
    "            for cluster_idx, _ in clusters.items():\n",
    "                rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "                validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "\n",
    "            if bool(np.all(validcorr)):\n",
    "                break   \n",
    "        \n",
    "        #corrs0 = np.array([1, x, *np.random.uniform(-1, 1, 4)])\n",
    "        linear_constraints = []\n",
    "\n",
    "        #Linear Constraint for each rho to be between 0 and 1\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        #Set bounds for the local minimisation \n",
    "        #limit correlations to [1, 1-pt, [-1,1], [-1,1], ...]\n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1,*[-1]*(len(clusters)-2)],\n",
    "                              [1, FIXED_CORR_1,*[1]*(len(clusters)-2)],\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 5000,\n",
    "                   'xtol'    : 1e-9,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        \n",
    "        minimizer_kwargs = {'args':(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                            'method': 'trust-constr',\n",
    "                            'options': options,\n",
    "                            'jac': F_jacobian, 'hess': F_hessian,\n",
    "                            'constraints' : [*linear_constraints, \n",
    "                                             {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1], 'hess': 0},\n",
    "                                             {'fun': constraint_zero, 'type':'eq','hess': 0},\n",
    "                                             #{'fun': constraint_rhos_sum, 'type': 'eq', 'args': [vmat, clusters, configcoef,]},\n",
    "                                            ],\n",
    "                            'bounds': bounds_corrs,\n",
    "                           }\n",
    "        \n",
    "        #Bounds for trial correlations\n",
    "        mybounds = MyBounds(xmax=[1, FIXED_CORR_1,*[1]*(len(clusters)-2)], \n",
    "                            xmin=[1, FIXED_CORR_1,*[-1]*(len(clusters)-2)]\n",
    "                           )\n",
    "        \n",
    "        mytakestep = MyTakeStep(vmat,\n",
    "                                clusters,\n",
    "                                stepsize=0.05\n",
    "                               )\n",
    "        \n",
    "        res = basinhopping(F, \n",
    "                           corrs0, #first guess\n",
    "                           niter=1000, #total num of iterations\n",
    "                           T=0.01, #temp for Metropolis MC trial search\n",
    "                           #stepsize=0.1,\n",
    "                           minimizer_kwargs=minimizer_kwargs,\n",
    "                           niter_success=10, #num iters to exit after no new minima found \n",
    "                           interval=5, #num iters to change step size\n",
    "                           disp=True,\n",
    "                           #accept_test=mybounds,\n",
    "                           take_step=mytakestep,\n",
    "                           #seed=42,\n",
    "                           #callback=basin_hopping_callback\n",
    "                          )\n",
    "\n",
    "        #Code to extract rhos and check if they sum them to 1 for sanity. Not used.\n",
    "        for cluster_idx in clusters.keys():\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "        \n",
    "        results_basinhopping = results_basinhopping.append({'T' : temp, \n",
    "                                                     '1-point_corr' : x, \n",
    "                                                     'F' : res.fun, \n",
    "                                                     'corrs': res.x,\n",
    "                                                    }, \n",
    "                                                    ignore_index = True\n",
    "                                                   )\n",
    "\n",
    "#save results\n",
    "results_basinhopping.to_pickle(f'results/bh_{eci[2]}_{MAX_TEMP}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Plot Phase Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:08:24.036119Z",
     "start_time": "2021-09-15T19:08:23.974295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for T in results_basinhopping['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_basinhopping[results_basinhopping['T'] == T]['1-point_corr'],\n",
    "                             y = results_basinhopping[results_basinhopping['T'] == T]['F'],\n",
    "                             mode='lines+markers',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F vs 1-point Corr\",\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"F\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot Pair Correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T18:11:26.033136Z",
     "start_time": "2021-09-16T18:11:25.709737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = results_basinhopping[results_basinhopping['1-point_corr'] == 0.0]['T'],\n",
    "                         y = results_basinhopping[results_basinhopping['1-point_corr'] == 0.0]['corrs'].str[2],\n",
    "                         mode='markers+lines',\n",
    "                        )\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"T vs 2-point Corr\",\n",
    "    xaxis_title=\"T\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fix 1-point and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:03:26.903822Z",
     "start_time": "2021-10-18T19:03:26.890428Z"
    },
    "code_folding": [
     0,
     4,
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def singlet_doublet_validpts(triangle = np.array([\n",
    "    [0, 0],\n",
    "    [-1, 1],\n",
    "    [1, 1],\n",
    "])):\n",
    "    \n",
    "    def uniform_triangle(u, v):\n",
    "        while True:\n",
    "            s = random.random()\n",
    "            t = random.random()\n",
    "            in_triangle = s + t <= 1\n",
    "            p = s * u + t * v if in_triangle else (1 - s) * u + (1 - t) * v\n",
    "            yield p\n",
    "\n",
    "    it = uniform_triangle(\n",
    "        triangle[1] - triangle[0],\n",
    "        triangle[2] - triangle[0],\n",
    "    )\n",
    "\n",
    "    points = np.array(list(itertools.islice(it, 0, 100)))\n",
    "    points += triangle[0]\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:03:27.316533Z",
     "start_time": "2021-10-18T19:03:27.296286Z"
    },
    "code_folding": [
     0,
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1, num = 10):\n",
    "    \n",
    "    def isInside(x, y, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1,):\n",
    "    \n",
    "        def area(x1, y1, x2, y2, x3, y3):\n",
    "            return abs((x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2))/2.0)\n",
    "\n",
    "        # Calculate area of triangle ABC\n",
    "        A = area(x1, y1, x2, y2, x3, y3)\n",
    "        # Calculate area of triangle PBC\n",
    "        A1 = area(x, y, x2, y2, x3, y3)\n",
    "        # Calculate area of triangle PAC\n",
    "        A2 = area(x1, y1, x, y, x3, y3)\n",
    "        # Calculate area of triangle PAB\n",
    "        A3 = area(x1, y1, x2, y2, x, y)\n",
    "        # Check if sum of A1, A2 and A3\n",
    "        # is same as A\n",
    "        if(A == A1 + A2 + A3):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "                                   \n",
    "\n",
    "    points = []\n",
    "    #xx, yy = np.meshgrid(xvalues, yvalues)\n",
    "    grid = np.meshgrid(xvalues, yvalues)\n",
    "    grid = np.vstack(list(map(np.ravel, grid))).T\n",
    "\n",
    "    for x, y in grid:\n",
    "        if isInside(x,y,x1=x1,y1=y1,x2=x2,y2=y2,x3=x3,y3=y3):\n",
    "            points.append([x,y])\n",
    "    \n",
    "    return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:03:31.502970Z",
     "start_time": "2021-10-18T19:03:31.299730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#singlet_doublet_pairs = singlet_doublet_validpts()\n",
    "num = 8\n",
    "xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "yvalues = np.linspace(0,1,num)\n",
    "singlet_doublet_pairs_1 = get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1, num = num)\n",
    "\n",
    "xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "yvalues = np.linspace(0,-1,num)\n",
    "singlet_doublet_pairs_2 = get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=-1, x3=1, y3=-1,num=num)\n",
    "\n",
    "xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "yvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "singlet_doublet_pairs_3 = get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=-1, x2=-1, y2=1, x3=1, y3=1,num=num)\n",
    "singlet_doublet_pairs_3 = np.append(singlet_doublet_pairs_3,[[0,-1+np.finfo(float).eps]],axis=0)\n",
    "\n",
    "singlet_doublet_pairs = np.vstack([np.array(singlet_doublet_pairs_1),np.array(singlet_doublet_pairs_2)])\n",
    "plt.style.use('default')\n",
    "plt.scatter(singlet_doublet_pairs_3[:, 0], singlet_doublet_pairs_3[:, 1], s=5)\n",
    "print(len(singlet_doublet_pairs_3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:27:07.131374Z",
     "start_time": "2021-10-18T19:07:02.219079Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_blanket = pd.DataFrame(columns = ['T', '1-point_corr', '2-point_corr', 'F','corrs'])\n",
    "NUM_TRIALS = 50\n",
    "MAX_TEMP = 500\n",
    "\n",
    "#corrsrange = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,21)\n",
    "#singlet_doublet_pairs = singlet_doublet_validpts()\n",
    "#singlet_doublet_pairs = get_uniform_traingular_points(num=20)\n",
    "print(f'Working ECIs: {eci}')\n",
    "for temp in tqdm(np.linspace(0, MAX_TEMP, num=11)):\n",
    "    for FIXED_CORR_1, FIXED_CORR_2 in tqdm(singlet_doublet_pairs_3):\n",
    "        \n",
    "        print('\\n=============================================================')\n",
    "        print(f'Working on 1-point corr: {FIXED_CORR_1:.4f}, NN corr: {FIXED_CORR_2:.4f}, Temp: {temp}')\n",
    "        print('=============================================================\\n')\n",
    "\n",
    "        MIN_RES_VAL = 1e5 #random large number\n",
    "        MIN_RES = corrs0\n",
    "\n",
    "        linear_constraints = []\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "\n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "\n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1, FIXED_CORR_2, *[-1]*(len(clusters)-3)],\n",
    "                              [1, FIXED_CORR_1, FIXED_CORR_2, *[1]*(len(clusters)-3)]\n",
    "                             )\n",
    "\n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 3000,\n",
    "                   'xtol'    : 1e-15,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "\n",
    "        for _ in tqdm(range(NUM_TRIALS)):\n",
    "\n",
    "            corrs0 = np.array([1, FIXED_CORR_1, FIXED_CORR_2, *np.random.uniform(-1, 1, len(clusters)-3)])\n",
    "\n",
    "            res = minimize(F,\n",
    "                           corrs0,\n",
    "                           method='trust-constr',\n",
    "                           args=(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                           options=options,\n",
    "                           #jac='3-point',\n",
    "                           #hess=BFGS(),\n",
    "                           jac=F_jacobian,\n",
    "                           hess=F_hessian,\n",
    "                           constraints=[*linear_constraints, \n",
    "                                        {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1]},\n",
    "                                        {'fun': constraint_zero, 'type':'eq',},\n",
    "                                        {'fun': constraint_NN, 'type':'eq','args':[FIXED_CORR_2]}\n",
    "                                       ],\n",
    "                           bounds=bounds_corrs,\n",
    "                          )\n",
    "\n",
    "            if res.fun < MIN_RES_VAL:\n",
    "                MIN_RES = res\n",
    "                MIN_RES_VAL = res.fun\n",
    "                print(f\"Found new minimum for Corr1:{FIXED_CORR_1:.4f}, Corr2:{FIXED_CORR_2:.4f} fun: {MIN_RES_VAL:.15f}\")\n",
    "                print(f'Current minimum correlations: {res.x}')\n",
    "#                     for cluster_idx, _ in clusters.items():\n",
    "#                         rho = np.matmul(vmat[cluster_idx],res.x)\n",
    "#                         print(rho)\n",
    "\n",
    "\n",
    "        break_next = False\n",
    "        for cluster_idx in clusters.keys():\n",
    "            try:\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "            except AssertionError:\n",
    "                break_next = True\n",
    "        if break_next:\n",
    "            print('No valid solution found')\n",
    "            continue\n",
    "\n",
    "        results_blanket = results_blanket.append({'T' : temp, \n",
    "                                           '1-point_corr' : FIXED_CORR_1,\n",
    "                                           '2-point_corr' : FIXED_CORR_2,\n",
    "                                           'F' : MIN_RES.fun, \n",
    "                                           'corrs': MIN_RES.x,\n",
    "                                          }, \n",
    "                                          ignore_index = True\n",
    "                                         )\n",
    "        #print(results_blanket)\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H:%M\")\n",
    "results_blanket.to_pickle(f'results/blanket_{eci[2]}_{eci[3]}_{MAX_TEMP}.pickle')\n",
    "results_blanket.to_pickle('temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:02:48.014002Z",
     "start_time": "2021-10-09T00:02:48.006527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "now = now.strftime(\"%d\")\n",
    "results_blanket.to_pickle(f'results/blanket_{eci[2]}_{MAX_TEMP}_{now}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:02:59.166506Z",
     "start_time": "2021-10-09T00:02:59.153758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_blanket['T'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:03:01.178654Z",
     "start_time": "2021-10-09T00:03:00.986579Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = 1000\n",
    "results_blanket[\n",
    "                (results_blanket['T'] == temp)\n",
    "               ]\n",
    "\n",
    "Xs = results_blanket[results_blanket['T'] == temp]['1-point_corr'].values\n",
    "Ys = results_blanket[results_blanket['T'] == temp]['2-point_corr'].values\n",
    "Zs = results_blanket[results_blanket['T'] == temp]['F'].values\n",
    "\n",
    "fig, ax2 = plt.subplots(nrows=1,figsize=(8, 6), dpi=100)\n",
    "plt.style.use('ggplot')\n",
    "ax2.set_title(f'T = {temp}')\n",
    "ax2.tricontour(Xs, Ys, Zs, levels=10, linewidths=0.5,)\n",
    "cntr2 = ax2.tricontourf(Xs, Ys, Zs, levels=14,)\n",
    "\n",
    "fig.colorbar(cntr2, ax=ax2,label='F')\n",
    "ax2.plot(Xs, Ys, 'ko', ms=3)\n",
    "ax2.set(xlim=(-1, 1), ylim=(-1, 1))\n",
    "\n",
    "plt.xlabel(\"1-point Correlation\")\n",
    "plt.ylabel(\"2-point NN Correlation\")\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T22:35:45.505484Z",
     "start_time": "2021-10-07T22:35:45.497025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T11:05:11.570692Z",
     "start_time": "2021-09-28T11:05:11.563006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_blanket['T'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T11:12:56.455252Z",
     "start_time": "2021-09-28T11:12:56.288308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "#fig.add_trace(traces)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    template='ggplot2',\n",
    "    legend_title=\"Legend Title\",\n",
    ")\n",
    "for temp in [0,150,200,300,450]:#[0.,  50., 100., 150., 200., 250., 300., 350., 400., 450., 500.]:\n",
    "    Xs = results_blanket[results_blanket['T'] == temp]['1-point_corr'].values\n",
    "    Ys = results_blanket[results_blanket['T'] == temp]['2-point_corr'].values\n",
    "    Zs = results_blanket[results_blanket['T'] == temp]['F'].values\n",
    "    points2D = np.vstack([Xs,Ys]).T\n",
    "    tri = Delaunay(points2D)\n",
    "\n",
    "    simplices = tri.simplices\n",
    "    temp_trace = ff.create_trisurf(x=Xs, y=Ys, z=Zs,\n",
    "                                   simplices=simplices,\n",
    "                                   title=\"F vs singlet-NN-corrlations\",\n",
    "                                   show_colorbar=False,\n",
    "                                   width=1000,\n",
    "                                   height=1000,\n",
    "                                   #aspectratio=dict(x=1, y=1, z=10)\n",
    "                                  )\n",
    "    fig.add_traces([temp_trace.data[0]])\n",
    "\n",
    "fig.layout.scene.xaxis.title='1-point Correlation'\n",
    "fig.layout.scene.yaxis.title='2-point Correlation'\n",
    "fig.layout.scene.zaxis.title='F'\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=750,\n",
    "    height=750,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fixed Chemical Potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T20:31:58.171680Z",
     "start_time": "2021-11-15T20:31:20.468574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_mu = pd.DataFrame(columns = ['T', 'mu', 'F','corrs'])\n",
    "NUM_TRIALS = 50\n",
    "MAX_TEMP = 1\n",
    "\n",
    "for temp in [0]:#tqdm(np.linspace(0, MAX_TEMP, num=11)):\n",
    "    for mu_1 in [0.25]:\n",
    "        \n",
    "        mu = np.array([-mu_1,mu_1])\n",
    "\n",
    "        MIN_RES_VAL = 1e5 #random large number\n",
    "        rho_pair = np.array([None,None])\n",
    "        MIN_RES = corrs0\n",
    "\n",
    "        linear_constraints = []\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            \n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        bounds_corrs = Bounds([1, *[-1]*(len(clusters)-1)],\n",
    "                              [1, *[1]*(len(clusters)-1)]\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 1,\n",
    "                   'maxiter' : 1000,\n",
    "                   'xtol'    : 1e-12,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        \n",
    "        for _ in tqdm(range(NUM_TRIALS)):\n",
    "            \n",
    "            corrs0 = np.array([1.   , *np.random.uniform(-1, 1, 5)])             \n",
    "#             for _ in iter(int,1): #infinite loop till a valid starting correlations are found\n",
    "#                 corrs0 = np.array([1, x, *np.random.uniform(-1, 1, len(clusters)-2)])\n",
    "#                 validcorr = np.ones(len(clusters), dtype=bool)\n",
    "        \n",
    "#                 for cluster_idx, _ in clusters.items():\n",
    "#                     rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "#                     validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "                \n",
    "#                 if bool(np.all(validcorr)):\n",
    "#                     break   \n",
    "                \n",
    "            res = minimize(F,\n",
    "                           corrs0,\n",
    "                           method='trust-constr',\n",
    "                           args=(vmat, kb, clusters, configs, configcoef,temp,eci,mu),\n",
    "                           options=options,\n",
    "                           #jac='3-point',\n",
    "                           #hess=BFGS(),\n",
    "                           jac=F_jacobian,\n",
    "                           hess=F_hessian,\n",
    "                           constraints=[*linear_constraints, \n",
    "                                        #{'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1]},\n",
    "                                        {'fun': constraint_zero, 'type':'eq',},\n",
    "                                       ],\n",
    "                           bounds=bounds_corrs,\n",
    "                          )\n",
    "            \n",
    "            if res.fun < MIN_RES_VAL:\n",
    "                MIN_RES = res\n",
    "                MIN_RES_VAL = res.fun\n",
    "                print(f\"Found new minimum for mu:{mu}, T:{temp} fun: {MIN_RES_VAL}\")\n",
    "                print(f'Current minimum correlations: {res.x}')\n",
    "        \n",
    "            \n",
    "        for cluster_idx in clusters.keys():\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "        \n",
    "        results_mu = results_mu.append({'T' : temp, \n",
    "                                           'mu' : mu_1, \n",
    "                                           'F' : MIN_RES.fun, \n",
    "                                           'corrs': MIN_RES.x,\n",
    "                                          }, \n",
    "                                          ignore_index = True\n",
    "                                         )\n",
    "        \n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H:%M\")\n",
    "#results_uniform.to_pickle(f'results/uni_{eci[2]},{eci[3]}_{MAX_TEMP}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Found new minimum for mu:[-0.25  0.25], T:0 fun: -0.14929162153260508\n",
    "Current minimum correlations: [1.   0.99 0.97 0.97 0.96 0.95]\n",
    "Found new minimum for mu:[0 0], T:0 fun: -0.05998629111609842\n",
    "Current minimum correlations: [ 1.  0. -0. -1. -0.  1.]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T01:31:55.471994Z",
     "start_time": "2021-11-16T01:31:55.438848Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#corrs0 = np.array([1.   , *np.random.uniform(-1, 1, 5)])             \n",
    "for _ in iter(int,1): #infinite loop till a valid starting correlations are found\n",
    "    corrs0 = np.array([1, *np.random.uniform(-1, 1, 5)])\n",
    "    validcorr = np.ones(len(clusters), dtype=bool)\n",
    "\n",
    "    for cluster_idx, _ in clusters.items():\n",
    "        rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "        validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "\n",
    "    if bool(np.all(validcorr)):\n",
    "        break\n",
    "print(corrs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
