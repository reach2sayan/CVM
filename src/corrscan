#!/usr/bin/env python3

import argparse
from energyfunctions import F, F_jacobian, F_hessian
from clusterdata import ClusterInfo
import numpy as np
import optimiser as opt
import json
from constraints import Constraints
from bounds import CorrBounds
from logger import Logger
import sys
import warnings
import random
import os

def get_uniform_triangular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1, num = 10):

    def isInside(x, y, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1,):

        def area(x1, y1, x2, y2, x3, y3):
            return abs((x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2))/2.0)

        # Calculate area of triangle ABC
        A = area(x1, y1, x2, y2, x3, y3)
        # Calculate area of triangle PBC
        A1 = area(x, y, x2, y2, x3, y3)
        # Calculate area of triangle PAC
        A2 = area(x1, y1, x, y, x3, y3)
        # Calculate area of triangle PAB
        A3 = area(x1, y1, x2, y2, x, y)
        # Check if sum of A1, A2 and A3
        # is same as A
        if(A == A1 + A2 + A3):
            return True
        else:
            return False


    points = []
    #xx, yy = np.meshgrid(xvalues, yvalues)
    grid = np.meshgrid(xvalues, yvalues)
    grid = np.vstack(list(map(np.ravel, grid))).T

    for x, y in grid:
        if isInside(x,y,x1=x1,y1=y1,x2=x2,y2=y2,x3=x3,y3=y3):
            points.append([x,y])

    return np.array(points)

if __name__ == '__main__':

    cwd = os.getcwd()
    datadir = f'{cwd}/data'
    outdir = f'{cwd}/results'
    np.set_printoptions(suppress=True,precision=2)
    np.random.seed(seed=42)    
    random.seed(42)
    kB = 8.617330337217213e-05

    parser = argparse.ArgumentParser()
    parser.add_argument('--eci',
                        default=f'{datadir}/eci.out', 
                        help="file containing ECI's (default: %(default)s)",
                       )
    parser.add_argument('--vmat', 
                        default=f'{datadir}/vmat.out',
                        help="file containing the vmatrix (default: %(default)s)",
                       )
    parser.add_argument('--clusters',
                        default=f'{datadir}/clusters.out',
                        help="file contain the maximal cluster description (default: %(default)s)",
                       )
    parser.add_argument('--kb',
                        default=f'{datadir}/kb.out',
                        help="file containing the kikuchi-barker coefficients (default: %(default)s)",
                       )
    parser.add_argument('--configcoef',
                        default=f'{datadir}/configcoef.out',
                        help="file containing coefficient for each subcluster (default: %(default)s)",
                       )
    parser.add_argument('--configs','--config',
                        default=f'{datadir}/config.out',
                        help="file containing subcluster descriptions (default: %(default)s)",
                       )
    parser.add_argument('--Tmin',
                        type=float,
                        default=0,
                        help="minimum T for phase diagram (default: %(default)s)",
                       )
    parser.add_argument('--Tmax',
                        type=float,
                        default=500.0,
                        help="maximum T for phase diagram (default: %(default)s)",
                       )
    parser.add_argument('--nTemp',
                        type=int,
                        default=11,
                        help="Number of data points between Tmin and Tmax (default: %(default)s)",
                       )
    parser.add_argument('--ncorr',
                        type=int,
                        default=8,
                        help="Number of data points 1-point correlations  (default: %(default)s)",
                       )
    parser.add_argument('--out',
                        default=f'{outdir}/corrscan_latest.csv',
                        help="Name of the dataframe containing the result of the optimisation"
                       )
    parser.add_argument('--verbose', '-v', action='count', default=0,
                        help="Indicate the verbosity of the fit (default: %(default)s)",
                       )
    parser.add_argument('--maxiter',
                        default=3000,
                        type=int,
                        help="Indicate maximum iterations for the local optimiser (default: %(default)s)",
                       )
    parser.add_argument('--xtol',
                        default=1e-12,
                        type=float,
                        help="Indicate the acceptable difference between two iterations (default: %(default)s)",
                       )
    parser.add_argument('--gtol',
                        default=1e-12,
                        type=float,
                        help="Indicate the acceptable violation of constraints (default: %(default)s)",
                       )
    parser.add_argument('--global_trials',
                        default=100,
                        type=int,
                        help="Indicate the number of initial point iteration for global minima search (default: %(default)s)",
                       )
    parser.add_argument('--uniform',
                        action='store_true',
                        default=True,
                        help="Use uniform sampling to search global minima (default: %(default)s)",
                       )
    parser.add_argument('--basinhopping',
                        action='store_true',
                        default=False,
                        help="Use basin hopping to search global minima",
                       )
    parser.add_argument('--show_warning',
                        action='store_true',
                        default=False,
                        help="Enables to show warning",
                       )
    parser.add_argument('--toscreen',
                        action='store_true',
                        default=False,
                        help="Enable logging to screen",
                       )
    parser.add_argument('--logfile',
                        default=f'{cwd}/log.out',
                        help="Filename for the log file (default: %(default)s)"
                       )

    args = parser.parse_args()

    if not args.show_warning:
        warnings.filterwarnings("ignore")

    sys.stdout = Logger(sys.stdout,args.logfile,args.toscreen)

    clusters = ClusterInfo(args.clusters,
                           args.kb,
                           args.configcoef,
                           args.configs,
                           args.vmat,
                           args.eci,
                          )
    outfile = args.out
    MIN_TEMP = args.Tmin
    MAX_TEMP = args.Tmax
    steps = args.nTemp
    NUM_TRIALS = args.global_trials
    num_clusters = len(clusters.clusters)   
    ncorr = args.ncorr

#    results_corrscan = pd.DataFrame(columns = ['T', '1-point_corr', '2-point_corr', 'F','corrs'])
    results_corrscan = []

    constraints = Constraints(clusters.clusters, 
                              clusters.configcoef, 
                              clusters.vmat)

    bounds = CorrBounds(num_clusters)

    options = {'verbose' : args.verbose,
               'maxiter' : args.maxiter,
               'xtol'    : args.xtol,
               'gtol'    : args.gtol,
               'initial_constr_penalty' : 10
              }

    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
    print("Cluster Parameters: ")
    print(clusters)
    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n")

    xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,ncorr)
    yvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,ncorr)
    singlet_doublet_pairs = get_uniform_triangular_points(xvalues, yvalues, 
                                                          x1=0, y1=-1, 
                                                          x2=-1, y2=1,
                                                          x3=1, y3=1,
                                                          num=ncorr
                                                         )
    singlet_doublet_pairs = np.append(singlet_doublet_pairs,
                                      [[0,-1+np.finfo(float).eps]],
                                      axis=0)

    import matplotlib.pyplot as plt
    plt.style.use('default')
    plt.scatter(singlet_doublet_pairs[:, 0], singlet_doublet_pairs[:, 1], s=5)
    plt.savefig('NN-grid.png',dpi=300)

    for temp in np.linspace(MIN_TEMP, MAX_TEMP, num=steps):
        for FIXED_CORR_1, FIXED_CORR_2 in singlet_doublet_pairs:
            
            print('-------------------------------------------------------------------')
            print(f" ## T: {temp} -- CORR_1: {FIXED_CORR_1}, -- CORR_2: {FIXED_CORR_2}")
            print('-------------------------------------------------------------------')

            constraints_corrscan = constraints.get_constraints_corrscan(FIXED_CORR_1, FIXED_CORR_2)
            bounds_corrscan = bounds.get_corrscan_bounds(FIXED_CORR_1,FIXED_CORR_2)

            result = opt.fit(F=F,
                             vmat=clusters.vmat, kb=clusters.kb, 
                             clusters=clusters.clusters, 
                             configs=clusters.configs, 
                             configcoef=clusters.configcoef,
                             eci=clusters.eci, 
                             temp=temp, 
                             options=options,
                             jac=F_jacobian,
                             hess=F_hessian,
                             NUM_TRIALS=NUM_TRIALS,
                             FIXED_CORR_1=FIXED_CORR_1,
                             FIXED_CORR_2=FIXED_CORR_2,
                             bounds=bounds_corrscan,
                             constraints=constraints_corrscan,
                             num_clusters=num_clusters,
                             NN=True
                            )

            print(f"## Corrs: {result.x} -- ENERGY: {result.fun}")
            print(f"## Rho:")
            print(f"{clusters.rho(result.x)}")

            clusters.check_result_validity(result.x)

            results_corrscan.append({'T' : temp, 
                                     '1-point_corr' : FIXED_CORR_1, 
                                     '2-point_corr': FIXED_CORR_2,
                                     'F' : result.fun, 
                                     'corrs': list(result.x),
                                    }, 
                                   )

    corrcols = [f'corr {i}' for i in range(num_clusters)]

    for item in results_corrscan:
        for idx, corrnum in enumerate(corrcols):
            item[f'{corrnum}'] = item['corrs'][idx]
    with open(outfile,'w') as fout:
        json.dump(results_corrscan, fout)
