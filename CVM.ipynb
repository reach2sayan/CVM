{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:13:13.272063Z",
     "start_time": "2021-08-31T16:13:12.707346Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import re #Used to read chunks from the data files - clusters, vmat, etec.\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from scipy.optimize import SR1, BFGS\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "from ase import Atoms\n",
    "from ase.units import kB\n",
    "from ase.visualize import view\n",
    "from ase.build import bulk\n",
    "from ase.spacegroup import crystal\n",
    "from ase.build import make_supercell\n",
    "from ase.io.vasp import write_vasp\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "pattern1 = re.compile(\"\\n\\n\\n\")\n",
    "pattern2 = re.compile(\"\\n\\n\")\n",
    "np.set_printoptions(suppress=True,precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all necessary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Clusters.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:13:16.027581Z",
     "start_time": "2021-08-31T16:13:16.013298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read clusters.out\n",
    "clusters = {}\n",
    "\n",
    "with open('clusters.out','r') as fclusters:\n",
    "    temp_clusters = fclusters.read().split('\\n\\n') #Read blocks separated by 1 empty line\n",
    "\n",
    "for idx, cluster in enumerate(temp_clusters):\n",
    "    if cluster == '': #Check for spurious empty blocks\n",
    "        continue \n",
    "    line = cluster.split('\\n') #If not empty split by lines\n",
    "    multiplicity = int(line[0]) #1st line\n",
    "    length = float(line[1]) #largest distance between two atoms\n",
    "    num_points = int(line[2]) #type of cluster\n",
    "    clusters[idx] = {'mult':multiplicity, 'length':length, 'type':num_points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:13:16.466326Z",
     "start_time": "2021-08-31T16:13:16.458226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'mult': 1, 'length': 0.0, 'type': 0},\n",
       " 1: {'mult': 1, 'length': 0.0, 'type': 1},\n",
       " 2: {'mult': 4, 'length': 0.86603, 'type': 2},\n",
       " 3: {'mult': 3, 'length': 1.0, 'type': 2},\n",
       " 4: {'mult': 12, 'length': 1.0, 'type': 3},\n",
       " 5: {'mult': 6, 'length': 1.0, 'type': 4}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Config.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:16:04.365120Z",
     "start_time": "2021-08-31T16:16:04.346135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read config.out\n",
    "configs = {}\n",
    "\n",
    "fconfig = open('config.out','r')\n",
    "_ = next(fconfig) #Ignore first line\n",
    "\n",
    "temp_config = fconfig.read()#.split('\\n\\n')\n",
    "temp_config = pattern1.split(temp_config) #split lines separated by 2 empty lines\n",
    "\n",
    "for idx, config in enumerate(temp_config):\n",
    "    if config == '': #Check for spurious empty blocks\n",
    "        continue\n",
    "    num_points = int(config[0]) #number of subclusters\n",
    "    config = pattern2.split(config[2:]) #now split individual subclusters separated by 1 blank line\n",
    "    min_coords = []\n",
    "    for _ in range(num_points):\n",
    "        min_coords.append(config[_].split('\\n')[0])\n",
    "    configs[idx] = {'subclus': list(map(int,min_coords)), 'num_of_subclus': len(min_coords)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:16:04.792359Z",
     "start_time": "2021-08-31T16:16:04.777735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'subclus': [0], 'num_of_subclus': 1},\n",
       " 1: {'subclus': [1, 1], 'num_of_subclus': 2},\n",
       " 2: {'subclus': [2, 2, 2], 'num_of_subclus': 3},\n",
       " 3: {'subclus': [2, 2, 2], 'num_of_subclus': 3},\n",
       " 4: {'subclus': [3, 3, 3, 3, 3, 3], 'num_of_subclus': 6},\n",
       " 5: {'subclus': [4, 4, 4, 4, 4, 4], 'num_of_subclus': 6}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Kikuchi-Barker Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:16:58.521737Z",
     "start_time": "2021-08-31T16:16:58.507734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read kb.out\n",
    "\n",
    "kb = {}\n",
    "fkb = open('kb.out','r')\n",
    "_ = next(fkb) #ignore first line\n",
    "\n",
    "temp_kb = fkb.read()\n",
    "temp_kb = temp_kb.split('\\n') #split file linewise\n",
    "\n",
    "for idx, kbcoeff in enumerate(temp_kb):\n",
    "    if kbcoeff == '': #check for spurious empty blocks\n",
    "        continue\n",
    "    kb[idx] = float(kbcoeff)\n",
    "\n",
    "fkb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:16:58.989899Z",
     "start_time": "2021-08-31T16:16:58.981370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0, 1: -1.0, 2: 1.0, 3: 1.0, 4: -1.0, 5: 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Coefficient of subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:19:24.328498Z",
     "start_time": "2021-08-31T16:19:24.312058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read configcoeff.out\n",
    "configcoef = {}\n",
    "\n",
    "with open('configcoef.out','r') as fsubmult:\n",
    "    _ = next(fsubmult) #ignore first line\n",
    "    temp_submult = fsubmult.read() \n",
    "    temp_submult = pattern2.split(temp_submult) #split lines into blocks separated by 2 empty lines\n",
    "    \n",
    "for idx, submult in enumerate(temp_submult):\n",
    "    submult = submult.split('\\n') #split into number of subclusters \n",
    "    while(\"\" in submult) :\n",
    "        submult.remove(\"\") #remove empty blocks\n",
    "    configcoef[idx] = list(map(float,submult[1:])) #also ignore 1st line of each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:19:24.731839Z",
     "start_time": "2021-08-31T16:19:24.726167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1.0],\n",
       " 1: [1.0, 1.0],\n",
       " 2: [1.0, 2.0, 1.0],\n",
       " 3: [1.0, 2.0, 1.0],\n",
       " 4: [1.0, 2.0, 1.0, 1.0, 2.0, 1.0],\n",
       " 5: [1.0, 4.0, 2.0, 4.0, 4.0, 1.0],\n",
       " 6: [0.0, -1.0, 1.0, 1.0, -1.0, 1.0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:20:55.995011Z",
     "start_time": "2021-08-31T16:20:55.984807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read vmat.out\n",
    "vmat = {}\n",
    "with open('vmat.out') as fvmat:\n",
    "    _ = next(fvmat) #ignore first lie\n",
    "    temp_vmat = fvmat.read()\n",
    "    temp_vmat = pattern2.split(temp_vmat) #split by 2 empty lines i.e. maxclusters\n",
    "    \n",
    "    while(\"\" in temp_vmat) :\n",
    "        temp_vmat.remove(\"\") #remove empty blocks\n",
    "    \n",
    "    for clus_idx, mat in enumerate(temp_vmat):\n",
    "        mat = mat.split('\\n') #split by 1 empty line i.e. subclusters\n",
    "        mat_float = np.empty(list(map(int, mat[0].split(' '))))\n",
    "        for idx, row in enumerate(mat[1:]): #ignore first line\n",
    "            mat_float[idx] = list(map(float,row.split(' ')[:-1]))\n",
    "        \n",
    "        vmat[clus_idx] = mat_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:20:57.333011Z",
     "start_time": "2021-08-31T16:20:57.316730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[1., 0., 0., 0., 0., 0.]]),\n",
       " 1: array([[ 0.5, -0.5,  0. ,  0. ,  0. ,  0. ],\n",
       "        [ 0.5,  0.5,  0. ,  0. ,  0. ,  0. ]]),\n",
       " 2: array([[ 0.25, -0.5 ,  0.25,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.  , -0.25,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.5 ,  0.25,  0.  ,  0.  ,  0.  ]]),\n",
       " 3: array([[ 0.25, -0.5 ,  0.  ,  0.25,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.  ,  0.  , -0.25,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.5 ,  0.  ,  0.25,  0.  ,  0.  ]]),\n",
       " 4: array([[ 0.12, -0.38,  0.25,  0.12, -0.12,  0.  ],\n",
       "        [ 0.12, -0.12,  0.  , -0.12,  0.12,  0.  ],\n",
       "        [ 0.12,  0.12, -0.25,  0.12, -0.12,  0.  ],\n",
       "        [ 0.12, -0.12, -0.25,  0.12,  0.12,  0.  ],\n",
       "        [ 0.12,  0.12,  0.  , -0.12, -0.12,  0.  ],\n",
       "        [ 0.12,  0.38,  0.25,  0.12,  0.12,  0.  ]]),\n",
       " 5: array([[ 0.06, -0.25,  0.25,  0.12, -0.25,  0.06],\n",
       "        [ 0.06, -0.12,  0.  ,  0.  ,  0.12, -0.06],\n",
       "        [ 0.06,  0.  , -0.25,  0.12,  0.  ,  0.06],\n",
       "        [ 0.06,  0.  ,  0.  , -0.12,  0.  ,  0.06],\n",
       "        [ 0.06,  0.12,  0.  ,  0.  , -0.12, -0.06],\n",
       "        [ 0.06,  0.25,  0.25,  0.12,  0.25,  0.06]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ECI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:21:49.745931Z",
     "start_time": "2021-08-31T16:21:49.732358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read eci\n",
    "eci = {}\n",
    "\n",
    "with open('eci.out') as feci:\n",
    "    _ = next(feci) #Ignore first line\n",
    "    temp_eci = feci.read()\n",
    "    temp_eci = temp_eci.split('\\n') #split by line\n",
    "\n",
    "for idx, eci_val in enumerate(temp_eci):\n",
    "    if eci_val == '':\n",
    "        continue\n",
    "    eci[idx] = float(eci_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:21:50.284063Z",
     "start_time": "2021-08-31T16:21:50.274588Z"
    }
   },
   "outputs": [],
   "source": [
    "#In this notebook ECI's are declared manually\n",
    "eci = {0: 0, 1: 0, 2: 0.5, 3: 0.0, 4: 0, 5: 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring test Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:22:39.412846Z",
     "start_time": "2021-08-31T16:22:39.404937Z"
    }
   },
   "outputs": [],
   "source": [
    "#corrs = np.array([1.    , 0.0   , 0.25,0.25  , 0.125 , 0.0625])\n",
    "#corrs = np.array([ 1.0, 0,  0.08,  0.23,-0.00054,  0.165])\n",
    "#corrs = np.array([ 1.  , 0.0 ,  0.25 ,  -0.25 ,  0.125 ,  0.0])\n",
    "corrs1 = np.array([1., 1., 1., 1., 1., 1.]) #Pure B\n",
    "corrs0 = np.array([1., -1., 1., 1., -1., 1.]) #Pure A\n",
    "corrsrand = np.array([1.    , 0.0   , 0.25,0.25  , 0.125 , 0.0625]) # AB - sqs\n",
    "#np.array([1.0, 0.0, -0.01852, -0.1358, -0.01235, -0.06173])\n",
    "#np.array([ 1.,    0.,    0.04,  0.14, -0.07,  0.12])\n",
    "#np.array([1.    , 0.0   , 0.25,0.25  , 0.125 , 0.0625])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up F, F Jacobian and F Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup test temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:24:32.105743Z",
     "start_time": "2021-08-31T16:24:32.099488Z"
    }
   },
   "outputs": [],
   "source": [
    "T = 1/kB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Old Declaration (Kept for backup, Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:24:29.044442Z",
     "start_time": "2021-08-31T16:24:29.026307Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def F_old(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "\n",
    "    S = 0\n",
    "    H = 0\n",
    "    \n",
    "    def clus_prob(cluster_idx):\n",
    "        rho = np.matmul(vmat[cluster_idx],corrs)\n",
    "\n",
    "        return rho\n",
    "    \n",
    "    def inner_sum(cluster_idx):\n",
    "        isum = 0\n",
    "        rho = clus_prob(cluster_idx)\n",
    "        #print(rho)\n",
    "        for i in range(configs[cluster_idx]['num_of_subclus']):\n",
    "            try:\n",
    "                if rho[i] == 0:\n",
    "                    isum += configcoef[cluster_idx][i] * rho[i] * math.log(np.finfo(float).eps)\n",
    "                else:\n",
    "                    isum += configcoef[cluster_idx][i] * rho[i] * math.log(rho[i])\n",
    "            except ValueError as ve:\n",
    "                print('Math Domain Error. Check the validity of the correlations')\n",
    "                print(f'corrs: {corrs} \\n Correlation Sum: {rho[i]}')\n",
    "                pass\n",
    "        return isum \n",
    "            \n",
    "    for cluster_idx, cluster in clusters.items():\n",
    "        H += cluster['mult']*eci[cluster_idx]*corrs[cluster_idx]\n",
    "        S += kb[cluster_idx]*inner_sum(cluster_idx)\n",
    "        \n",
    "    #return kB*S\n",
    "    return H + kB*T*S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:24:41.189329Z",
     "start_time": "2021-08-31T16:24:41.172740Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0: 2.00 -- Corrs 1: 2.00 -- Corrs Rand: -2.13\n"
     ]
    }
   ],
   "source": [
    "f_old0 = F_old(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_old1 = F_old(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_rand = F_old(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0: {f_old0:.2f} -- Corrs 1: {f_old1:.2f} -- Corrs Rand: {f_rand:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Energy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:35:03.215333Z",
     "start_time": "2021-08-31T16:35:03.197515Z"
    }
   },
   "outputs": [],
   "source": [
    "def F(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    F = H + T*SUM(rho * log(rho))\n",
    "    \"\"\"\n",
    "    \n",
    "    H = 0\n",
    "    S = 0\n",
    "    \n",
    "    def get_corr_sum(corrs,vmat,cluster_index,config_idx):\n",
    "        \"\"\"\n",
    "        returns rho for a particular subcluster of a maxcluster\n",
    "        \"\"\"\n",
    "        #TODO: This loop can be reduced as a Dot Product\n",
    "        corrsum = 0\n",
    "        for corr_idx, corr in enumerate(corrs):\n",
    "            corrsum += vmat[cluster_idx][config_idx][corr_idx]*corr\n",
    "\n",
    "        return corrsum\n",
    "    \n",
    "    #Loop through all maximal clusters\n",
    "    for cluster_idx, cluster in clusters.items():\n",
    "        \n",
    "        H += cluster['mult']*eci[cluster_idx]*corrs[cluster_idx] #H is a func of maxclux only\n",
    "        \n",
    "        temp_S = 0\n",
    "        \n",
    "        #Loop through subclusters of each maximal cluster\n",
    "        for config_idx, config in enumerate(configcoef[cluster_idx]):\n",
    "            corrsum = get_corr_sum(corrs,vmat,cluster_idx,config_idx)\n",
    "            try:\n",
    "                if corrsum == 0:\n",
    "                    #If rho is 0, then replace with smallest +ve number as per machine precision\n",
    "                    #The log doesn't matter, cuz, although log of a small number is a finite number, the 0multiplicative factor makes it 0 any\n",
    "                    temp_S += config*corrsum*math.log(np.finfo(float).tiny)\n",
    "                else:\n",
    "                    temp_S += config*corrsum*math.log(corrsum)\n",
    "            except ValueError as ve:\n",
    "                pass\n",
    "                #Exception handling retained for possible -ve rhos\n",
    "                print('Math Domain Error. Check the validity of the correlations')\n",
    "                print(f'corrs: {corrs} \\n Correlation Sum: {corrsum}')\n",
    "                \n",
    "        S += kb[cluster_idx]*temp_S\n",
    "    \n",
    "    return H + kB*T*S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:35:03.635392Z",
     "start_time": "2021-08-31T16:35:03.627488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0: 2.00 -- Corrs 1: 2.00 -- Corrs Rand: -2.13\n"
     ]
    }
   ],
   "source": [
    "f0 = F(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f1 = F(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "frand = F(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0: {f0:.2f} -- Corrs 1: {f1:.2f} -- Corrs Rand: {frand:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Energy Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:53:06.002047Z",
     "start_time": "2021-08-31T16:53:05.978912Z"
    }
   },
   "outputs": [],
   "source": [
    "def F_jacobian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    Vector representation gradient of F with Corrs\n",
    "    [dF/dcorr0, dF/dcorr1, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_corr_sum(corrs,vmat,cluster_index,config_idx):\n",
    "        \"\"\"\n",
    "        returns rho for a particular subcluster of a maxcluster\n",
    "        \"\"\"\n",
    "        #TODO: This loop can be reduced as a Dot Product\n",
    "        corrsum = 0\n",
    "        for corr_idx, corr in enumerate(corrs):\n",
    "            corrsum += vmat[cluster_idx][config_idx][corr_idx]*corr\n",
    "        \n",
    "        return corrsum\n",
    "            \n",
    "    \n",
    "    F_jac = []\n",
    "    #Loop through each correlation\n",
    "    for corr_idx, corr in enumerate(corrs):\n",
    "        \n",
    "        temp_S_jac = 0\n",
    "        \n",
    "        #Loop through all maxclus, since the rhos depend on the correlation\n",
    "        for cluster_idx, cluster in clusters.items():\n",
    "            \n",
    "            temp_config_sum = 0\n",
    "            #Loop through all subclusters in a maximal cluster\n",
    "            for config_idx, config in enumerate(configcoef[cluster_idx]):\n",
    "                corrsum = get_corr_sum(corrs,vmat,cluster_idx,config_idx)\n",
    "                try:\n",
    "                    if corrsum == 0:\n",
    "                        #temp_config_sum += config*vmat[cluster_idx][config_idx][corr_idx]*(1 + np.log(corrsum))\n",
    "                        #^^ same line can be used since np.log(x) automaticaly returns -infty if x = 0\n",
    "                        #Can't use tiny, since it gives a very small value, but log of that value gives a finite value. Note that we took tiny in the F calculation but since it had another multiplicative factor which is also small, the finite log value didn't matter)\n",
    "                        temp_config_sum += np.NINF #negative infinity\n",
    "                        \n",
    "                    else:\n",
    "                        temp_config_sum += config*vmat[cluster_idx][config_idx][corr_idx]*(1 + np.log(corrsum))\n",
    "                except ValueError as ve:\n",
    "                    #Exception handling retained for possible -ve rhos\n",
    "                    print('Math Domain Error. Check the validity of the correlations')\n",
    "                    print(f'corrs: {corrs} \\n Correlation Sum: {corrsum}')\n",
    "                    pass\n",
    "            \n",
    "            temp_S_jac += kb[cluster_idx]*temp_config_sum\n",
    "        \n",
    "        F_jac.append(clusters[corr_idx]['mult']*eci[corr_idx] + kB*T*temp_S_jac)\n",
    "        \n",
    "    return np.array(F_jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:53:06.327114Z",
     "start_time": "2021-08-31T16:53:06.316441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0: [nan nan nan nan nan nan] \n",
      " Corrs 1: [nan nan nan nan nan nan] \n",
      " Corrs Rand [-1.9  -0.1   2.57  0.41  0.23 -0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-42d6047e7b4b>:57: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  temp_S_jac += kb[cluster_idx]*temp_config_sum\n"
     ]
    }
   ],
   "source": [
    "f_jaco0 = F_jacobian(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_jaco1 = F_jacobian(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_jacorand = F_jacobian(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0: {f_jaco0} \\n Corrs 1: {f_jaco1} \\n Corrs Rand {f_jacorand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Energy Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:55:22.474782Z",
     "start_time": "2021-08-31T16:55:22.451041Z"
    }
   },
   "outputs": [],
   "source": [
    "def F_hessian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    Vector representation gradient of F with Corrs\n",
    "    [[d^2F/dcorr0 dcorr0, d^2F/dcorr0 dcorr1, ..., d^2F/dcorr0 dcorrn],\n",
    "     [d^2F/dcorr1 dcorr0, d^2F/dcorr1 dcorr1, ..., d^2F/dcorr1 dcorrn],\n",
    "     .\n",
    "     .\n",
    "     .\n",
    "     [d^2F/dcorrn dcorr0, d^2F/dcorrn dcorr1, ..., d^2F/dcorrn dcorrn],\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    F_hess = np.empty([len(corrs),len(corrs)])\n",
    "    \n",
    "    def get_corr_sum(corrs,vmat,cluster_index,config_idx):\n",
    "        corrsum = 0\n",
    "        for corr_idx, corr in enumerate(corrs):\n",
    "            corrsum += vmat[cluster_idx][config_idx][corr_idx]*corr\n",
    "\n",
    "        return corrsum\n",
    "    \n",
    "    #Enuemrate through all possible correlation pairs\n",
    "    for corr_idx_1, corr_1 in enumerate(corrs):\n",
    "        for corr_idx_2, corr_2 in enumerate(corrs):\n",
    "            \n",
    "            temp_hess = 0\n",
    "            #loop through maximal clusters\n",
    "            for cluster_idx, cluster in clusters.items():\n",
    "                temp_config_sum = 0\n",
    "                #loop through subclustes for the present maximal cluster\n",
    "                for config_idx, config in enumerate(configcoef[cluster_idx]):\n",
    "                    corrsum = get_corr_sum(corrs,vmat,cluster_idx,config_idx)\n",
    "                    try:\n",
    "                        if corrsum == 0:\n",
    "                            temp_config_sum += config*vmat[cluster_idx][config_idx][corr_idx_1]*vmat[cluster_idx][config_idx][corr_idx_2]/np.finfo(float).tiny #dividing by a small number.\n",
    "                            #Note that you can also set the value as +ve infinity.\n",
    "                            #temp_config_sum += np.PINF\n",
    "                        else:\n",
    "                            temp_config_sum += config*vmat[cluster_idx][config_idx][corr_idx_1]*vmat[cluster_idx][config_idx][corr_idx_2]/corrsum\n",
    "                    except ValueError as ve:\n",
    "                        #Exception handling retained for possible -ve rhos\n",
    "                        print('Math Domain Error. Check the validity of the correlations')\n",
    "                        print(f'corrs: {corrs} \\n Correlation Sum: {corrsum}')\n",
    "                        pass\n",
    "                temp_hess += kb[cluster_idx]*temp_config_sum\n",
    "            F_hess[corr_idx_1][corr_idx_2] = kB*T*temp_hess\n",
    "    \n",
    "    return F_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T16:55:23.553393Z",
     "start_time": "2021-08-31T16:55:23.480829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0:\n",
      " [[ 3.34e+306 -1.40e+306 -2.11e+306 -2.46e+306  0.00e+000 -1.76e+305]\n",
      " [-1.40e+306  9.13e+306  4.21e+306  4.92e+306 -7.02e+305  7.02e+305]\n",
      " [-2.11e+306  4.21e+306  8.43e+306  0.00e+000  1.40e+306 -7.02e+305]\n",
      " [-2.46e+306  4.92e+306  0.00e+000  8.43e+306  7.02e+305 -3.51e+305]\n",
      " [ 0.00e+000 -7.02e+305  1.40e+306  7.02e+305  3.51e+306  7.02e+305]\n",
      " [-1.76e+305  7.02e+305 -7.02e+305 -3.51e+305  7.02e+305  2.63e+306]] \n",
      " Corrs 1:\n",
      " [[ 3.34e+306  1.40e+306 -2.11e+306 -2.46e+306  0.00e+000 -1.76e+305]\n",
      " [ 1.40e+306  9.13e+306 -4.21e+306 -4.92e+306 -7.02e+305 -7.02e+305]\n",
      " [-2.11e+306 -4.21e+306  8.43e+306  0.00e+000 -1.40e+306 -7.02e+305]\n",
      " [-2.46e+306 -4.92e+306  0.00e+000  8.43e+306 -7.02e+305 -3.51e+305]\n",
      " [ 0.00e+000 -7.02e+305 -1.40e+306 -7.02e+305  3.51e+306 -7.02e+305]\n",
      " [-1.76e+305 -7.02e+305 -7.02e+305 -3.51e+305 -7.02e+305  2.63e+306]]\n",
      " Corrs Rand:\n",
      " [[ 1.25  0.13 -0.55 -0.41 -0.16  0.14]\n",
      " [ 0.13  2.99  0.02  0.01 -0.89 -0.35]\n",
      " [-0.55  0.02  3.49 -0.97 -0.25 -0.69]\n",
      " [-0.41  0.01 -0.97  2.76 -0.13 -0.34]\n",
      " [-0.16 -0.89 -0.25 -0.13  1.94  0.27]\n",
      " [ 0.14 -0.35 -0.69 -0.34  0.27  1.29]]\n"
     ]
    }
   ],
   "source": [
    "f_hess0 = F_hessian(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hess1 = F_hessian(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hessrand = F_hessian(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0:\\n {f_hess0} \\n Corrs 1:\\n {f_hess1}\\n Corrs Rand:\\n {f_hessrand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T17:01:52.457322Z",
     "start_time": "2021-08-31T17:01:52.445923Z"
    }
   },
   "outputs": [],
   "source": [
    "def constraint_rhos_sum(corrs, vmat, clusters, configcoef,):\n",
    "    \"\"\"\n",
    "    Constraints the sum of each rho. As of now, it's done in a weird way, where the total sum of the array:\n",
    "    [1 - sum(rho), .... ] is constrained to sum to 0. This along with the constraint that each rho is between\n",
    "    0 and 1, seems to make it work. I think that by the this might be a redundant constraint as well.\n",
    "    \"\"\"\n",
    "    rho_sum = []\n",
    "\n",
    "    def clus_prob(cluster_idx):\n",
    "        rho = np.matmul(vmat[cluster_idx],corrs)\n",
    "        return rho\n",
    "    \n",
    "    for cluster_idx, _ in clusters.items():\n",
    "        rho = clus_prob(cluster_idx)\n",
    "        rho_sum.append(np.sum(configcoef[cluster_idx]*rho))\n",
    "    \n",
    "    return np.sum(1 - np.array(rho_sum))\n",
    "\n",
    "def constraint_singlet(corrs,FIXED_CORR_1):\n",
    "    \"\"\"\n",
    "    constrains the 1-pt correlation:\n",
    "    corrs[1] = FIXED_CORR_1\n",
    "    \"\"\"\n",
    "    return corrs[1] - FIXED_CORR_1   \n",
    "\n",
    "def constraint_zero(corrs):\n",
    "    \"\"\"\n",
    "    constrains the 1-pt correlation:\n",
    "    corrs[0] = 1\n",
    "    \"\"\"\n",
    "    return 1 - corrs[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T17:03:21.949280Z",
     "start_time": "2021-08-31T17:03:21.926385Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_random_corr(correlation):\n",
    "    \"\"\"\n",
    "    Returns a random set of correlations for a BCC binary alloy with a fixed 1-pt correlation\n",
    "    \"\"\"\n",
    "    concentration = (correlation - (-1))/(1 - (-1)) #scale factor\n",
    "    a=0.5\n",
    "    atoms = crystal(['Fe'], [(0,0,0)], spacegroup=229, \n",
    "                    cellpar=[1, 1, 1, 90, 90, 90])\n",
    "    \n",
    "    scell = make_supercell(atoms,np.eye(3)*3)\n",
    "    temp_atomic_nums = scell.get_atomic_numbers()\n",
    "    mask = np.zeros(len(temp_atomic_nums), dtype=int).astype(bool)\n",
    "    mask[:int(concentration*len(temp_atomic_nums))] = 1.0\n",
    "    np.random.shuffle(mask)\n",
    "    temp_atomic_nums[mask] = 13\n",
    "    scell.set_atomic_numbers(temp_atomic_nums)\n",
    "    \n",
    "    write_vasp('POSCAR.temp',scell,sort=True,direct=True)\n",
    "    _ = subprocess.call('./poscar2str',shell=True)\n",
    "    \n",
    "    corrs = subprocess.call('corrdump -c',shell=True)\n",
    "    \n",
    "    popen = subprocess.Popen('corrdump -c'.split(), stdout=subprocess.PIPE)\n",
    "    popen.wait()\n",
    "    return [float(corr) for corr in popen.stdout.read().decode(\"utf-8\").split('\\t')[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T15:23:17.259829Z",
     "start_time": "2021-08-31T15:23:17.103805Z"
    }
   },
   "outputs": [],
   "source": [
    "get_random_corr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T04:06:39.750471Z",
     "start_time": "2021-08-31T04:06:39.743013Z"
    }
   },
   "outputs": [],
   "source": [
    "def basin_hopping_callback(corrs, G, accept):\n",
    "    \"\"\"\n",
    "    Function to print diagnostic while fitting. \n",
    "    Not being used right now\n",
    "    \"\"\"\n",
    "    if accept == True:\n",
    "        clear_output(wait=True)\n",
    "        print(f'1-pt Correlation: {corrs[1]:.2f}')\n",
    "        print(f'Concentation: {(corrs[1] - (-1))/(1 - (-1)):.2f}')\n",
    "        print(f'New Minima found --> G: {G:.2f}')\n",
    "        print('Current Rho:')\n",
    "        for cluster_idx in clusters:\n",
    "            print(np.matmul(vmat[cluster_idx],corrs))\n",
    "        print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T17:04:39.843652Z",
     "start_time": "2021-08-31T17:04:39.833223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,  300.,  600.,  900., 1200., 1500., 1800., 2100., 2400.,\n",
       "        2700., 3000.]),\n",
       " array([   0.,  300.,  600.,  900., 1200., 1500., 1800., 2100., 2400.,\n",
       "        2700., 3000.]),\n",
       " array([-1.  , -0.88, -0.75, -0.62, -0.5 , -0.38, -0.25, -0.12,  0.  ,\n",
       "         0.12,  0.25,  0.38,  0.5 ,  0.62,  0.75,  0.88,  1.  ]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 3000, num=11), np.linspace(0, 3000, num=11),np.linspace(-1,1,17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basin Hopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T17:04:30.018423Z",
     "start_time": "2021-08-31T17:04:30.011412Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyBounds:\n",
    "    \"\"\"\n",
    "    Class to constrain the trail correlations of Basin Hopping\n",
    "    \"\"\"\n",
    "    def __init__(self, xmax=[1]*6, xmin=[-1]*6):\n",
    "        self.xmax = np.array(xmax)\n",
    "        self.xmin = np.array(xmin)\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        x = kwargs[\"x_new\"]\n",
    "        tmax = bool(np.all(x <= self.xmax))\n",
    "        tmin = bool(np.all(x >= self.xmin))\n",
    "\n",
    "        return tmax and tmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T13:06:42.890780Z",
     "start_time": "2021-08-31T12:55:01.251163Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_basinhopping = pd.DataFrame(columns = ['T', '1-point_corr', 'F','corrs'])\n",
    "\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H-%M\")\n",
    "FNAME = f'results-{now}.txt'\n",
    "fout = open(FNAME,'w')\n",
    "\n",
    "for temp in tqdm(np.linspace(0, 10000, num=11)):\n",
    "    for x in [0]:#tqdm(np.linspace(-1,1,9)):\n",
    "        clear_output(wait=True)\n",
    "        fout.write(f\"ECI {eci}\\n\")\n",
    "        fout.write(f\"Optimising for T: {temp}, 1-pt corr: {x}\\n\")\n",
    "        FIXED_CORR_1 = x\n",
    "\n",
    "        rho_pair = np.array([None,None])\n",
    "        \n",
    "        #Make a first guess\n",
    "        corrs0 = np.array([1, *np.random.uniform(-1, 1, 5)])\n",
    "        linear_constraints = []\n",
    "\n",
    "        #Linear Constraint for each rho to be between 0 and 1\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        #Set bounds for the local minimisation \n",
    "        #limit correlations to [1, 1-pt, [-1,1], [-1,1], ...]\n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1,*[-1]*(len(clusters)-2)],\n",
    "                              [1, FIXED_CORR_1,*[1]*(len(clusters)-2)]\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 5000,\n",
    "                   'xtol'    : 1e-15,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        \n",
    "        minimizer_kwargs = {'args':(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                            'method': 'trust-constr',\n",
    "                            'options': options,\n",
    "                            'jac': F_jacobian, 'hess': F_hessian,\n",
    "                            'constraints' : [{'fun': constraint_rhos_sum, 'type': 'eq', 'args': [vmat, clusters, configcoef,]},\n",
    "                                             *linear_constraints, \n",
    "                                             {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1]},\n",
    "                                             {'fun': constraint_zero, 'type':'eq',},\n",
    "                                            ],\n",
    "                            'bounds': bounds_corrs,\n",
    "                           }\n",
    "        \n",
    "        mybounds = MyBounds(xmax=[1, FIXED_CORR_1,*[1]*(len(clusters)-2)], xmin=[1, FIXED_CORR_1,*[-1]*(len(clusters)-2)])\n",
    "        \n",
    "        res = basinhopping(F, \n",
    "                           corrs0, #first guess\n",
    "                           niter=100, #total num of iterations\n",
    "                           T=1.0, #temp for Metropolis MC trial search\n",
    "                           stepsize=10,\n",
    "                           minimizer_kwargs=minimizer_kwargs,\n",
    "                           niter_success=20, #num iters to exit after no new minima found \n",
    "                           interval=5, #num iters to change step size\n",
    "                           disp=True,\n",
    "                           accept_test=mybounds,\n",
    "                           #callback=basin_hopping_callback\n",
    "                          )\n",
    "        \n",
    "        fout.write(f'1-pt Correlation: {res.x[1]:.2f}\\n')\n",
    "        fout.write(f'Concentation: {(res.x[1] - (-1))/(1 - (-1)):.2f}\\n')\n",
    "        fout.write(f'New Minima found --> G: {res.fun:.2f}\\n')\n",
    "        fout.write(f'Correlations: {res.x}\\n')\n",
    "        fout.write('Current Rho:\\n')\n",
    "        for cluster_idx in clusters:\n",
    "            fout.write(f'{np.matmul(vmat[cluster_idx],res.x)}\\n')\n",
    "        fout.write(\"===========================\\n\")\n",
    "        fout.flush()\n",
    "        os.fsync(fout.fileno())\n",
    "\n",
    "        #Code to extract rho for pair for sanity check. Not used.\n",
    "        for cluster_idx in clusters:\n",
    "            if cluster_idx == 2:\n",
    "                rho_pair = np.matmul(vmat[cluster_idx],res.x)\n",
    "                break\n",
    "        \n",
    "        results_basinhopping = results_basinhopping.append({'T' : temp, \n",
    "                                                     '1-point_corr' : x, \n",
    "                                                     'F' : res.fun, \n",
    "                                                     'corrs': res.x,\n",
    "                                                     'rhos': rho_pair\n",
    "                                                    }, \n",
    "                                                    ignore_index = True\n",
    "                                                   )\n",
    "\n",
    "#save results\n",
    "results_basinhopping.to_csv(f'phaseDiag_{now}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:56:54.320728Z",
     "start_time": "2021-08-31T14:56:54.253359Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for T in results_basinhopping['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_basinhopping[results_basinhopping['T'] == T]['1-point_corr'],\n",
    "                             y = results_basinhopping[results_basinhopping['T'] == T]['F'],\n",
    "                             mode='markers+lines',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"F vs 1-point Corr - {eci}\",\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"F\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:56:35.931669Z",
     "start_time": "2021-08-31T14:56:35.856311Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "#spec = \n",
    "for T in results_basinhopping['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_basinhopping[results_basinhopping['1-point_corr'] == 0.0]['T'],\n",
    "                             y = np.stack(results_basinhopping[results_basinhopping['1-point_corr'] == 0.0]['corrs'].values)[:,2],\n",
    "                             mode='markers+lines',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"T vs 2-point Corr: {eci}\",\n",
    "    xaxis_title=\"T\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:29.428465Z",
     "start_time": "2021-08-30T21:28:18.850358Z"
    }
   },
   "outputs": [],
   "source": [
    "results_uniform = pd.DataFrame(columns = ['T', '1-point_corr', 'F','corrs'])\n",
    "NUM_TRIALS = 500\n",
    "\n",
    "for temp in [3000]:#tqdm(np.linspace(0, 3000, num=11)):\n",
    "    for x in tqdm(np.linspace(-1,1,17)):\n",
    "        FIXED_CORR_1 = x\n",
    "        \n",
    "        MIN_RES_VAL = 1e5 #random large number\n",
    "        rho_pair = np.array([None,None])\n",
    "        #corrs0 = np.array([1, *np.random.uniform(-1, 1, 5)])\n",
    "        corrs0 = np.array([1, *np.random.uniform(-1, 1, 2)])\n",
    "        MIN_RES = corrs0\n",
    "\n",
    "        linear_constraints = []\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1,*[-1]*(len(clusters)-2)],\n",
    "                              [1, FIXED_CORR_1,*[1]*(len(clusters)-2)]\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 5000,\n",
    "                   'xtol'    : 1e-15,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        \n",
    "        for _ in tqdm(range(NUM_TRIALS)):\n",
    "            \n",
    "            res = minimize(F,\n",
    "                           corrs0,\n",
    "                           method='trust-constr',\n",
    "                           args=(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                           options=options,\n",
    "                           jac=F_jacobian, hess=F_hessian,\n",
    "                           constraints=[{'fun': constraint_rhos_sum, 'type': 'eq', 'args': [vmat, clusters, configcoef,],'hess': 0},\n",
    "                                        *linear_constraints, \n",
    "                                        {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1]},\n",
    "                                        {'fun': constraint_zero, 'type':'eq',},\n",
    "                                       ],\n",
    "                           bounds=bounds_corrs,\n",
    "                          )\n",
    "            \n",
    "            if res.fun < MIN_RES_VAL:\n",
    "                MIN_RES = res\n",
    "                MIN_RES_VAL = res.fun\n",
    "                print(f\"Found new minimum for x:{x}, T:{temp} fun: {MIN_RES_VAL}\")\n",
    "        \n",
    "            \n",
    "        for cluster_idx in clusters:\n",
    "            if cluster_idx == 2:\n",
    "                rho_pair = np.matmul(vmat[cluster_idx],MIN_RES.x)\n",
    "                break\n",
    "        \n",
    "        results_uniform = results_uniform.append({'T' : temp, \n",
    "                                           '1-point_corr' : x, \n",
    "                                           'F' : MIN_RES.fun, \n",
    "                                           'corrs': MIN_RES.x,\n",
    "                                           'rhos': rho_pair\n",
    "                                          }, \n",
    "                                          ignore_index = True\n",
    "                                         )\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H:%M\")\n",
    "results_uniform.to_csv(f'phaseDiag_{now}_uniform.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:27:04.091942Z",
     "start_time": "2021-08-30T21:27:03.622783Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for T in results_uniform['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_uniform[results_uniform['T'] == T]['1-point_corr'],\n",
    "                             y = results_uniform[results_uniform['T'] == T]['F'],\n",
    "                             mode='markers+lines',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F vs 1-point Corr\",\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"F\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
