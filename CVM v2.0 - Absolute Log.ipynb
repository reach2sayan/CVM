{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.313346Z",
     "start_time": "2021-10-19T03:53:08.040873Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import re #Used to read chunks from the data files - clusters, vmat, etec.\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from scipy.optimize import SR1, BFGS\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "from ase import Atoms\n",
    "from ase.units import kB\n",
    "from ase.visualize import view\n",
    "from ase.build import bulk\n",
    "from ase.spacegroup import crystal\n",
    "from ase.build import make_supercell\n",
    "from ase.io.vasp import write_vasp\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "pattern1 = re.compile(\"\\n\\n\\n\")\n",
    "pattern2 = re.compile(\"\\n\\n\")\n",
    "np.set_printoptions(suppress=True,precision=2)\n",
    "random.seed(42)\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all necessary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read clusters.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.321107Z",
     "start_time": "2021-10-19T03:53:09.315668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read clusters.out\n",
    "clusters = {}\n",
    "\n",
    "with open('clusters.out','r') as fclusters:\n",
    "    temp_clusters = fclusters.read().split('\\n\\n') #Read blocks separated by 1 empty line\n",
    "\n",
    "for idx, cluster in enumerate(temp_clusters):\n",
    "    if cluster == '': #Check for spurious empty blocks\n",
    "        continue \n",
    "    line = cluster.split('\\n') #If not empty split by lines\n",
    "    multiplicity = int(line[0]) #1st line\n",
    "    length = float(line[1]) #largest distance between two atoms\n",
    "    num_points = int(line[2]) #type of cluster\n",
    "    clusters[idx] = {'mult':multiplicity, 'length':length, 'type':num_points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.338378Z",
     "start_time": "2021-10-19T03:53:09.323880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'mult': 1, 'length': 0.0, 'type': 0},\n",
       " 1: {'mult': 1, 'length': 0.0, 'type': 1},\n",
       " 2: {'mult': 4, 'length': 0.86603, 'type': 2},\n",
       " 3: {'mult': 3, 'length': 1.0, 'type': 2},\n",
       " 4: {'mult': 12, 'length': 1.0, 'type': 3},\n",
       " 5: {'mult': 6, 'length': 1.0, 'type': 4}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.348799Z",
     "start_time": "2021-10-19T03:53:09.340954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read config.out\n",
    "configs = {}\n",
    "\n",
    "fconfig = open('config.out','r')\n",
    "_ = next(fconfig) #Ignore first line\n",
    "\n",
    "temp_config = fconfig.read()#.split('\\n\\n')\n",
    "temp_config = pattern1.split(temp_config) #split lines separated by 2 empty lines\n",
    "\n",
    "for idx, config in enumerate(temp_config):\n",
    "    if config == '': #Check for spurious empty blocks\n",
    "        continue\n",
    "    num_points = int(config[0]) #number of subclusters\n",
    "    config = pattern2.split(config[2:]) #now split individual subclusters separated by 1 blank line\n",
    "    min_coords = []\n",
    "    for _ in range(num_points):\n",
    "        min_coords.append(config[_].split('\\n')[0])\n",
    "    configs[idx] = {'subclus': list(map(int,min_coords)), 'num_of_subclus': len(min_coords)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Kikuchi-Baker coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.361388Z",
     "start_time": "2021-10-19T03:53:09.351273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read kb.out\n",
    "\n",
    "kb = {}\n",
    "fkb = open('kb.out','r')\n",
    "_ = next(fkb) #ignore first line\n",
    "\n",
    "temp_kb = fkb.read()\n",
    "temp_kb = temp_kb.split('\\n') #split file linewise\n",
    "\n",
    "for idx, kbcoeff in enumerate(temp_kb):\n",
    "    if kbcoeff == '': #check for spurious empty blocks\n",
    "        continue\n",
    "    kb[idx] = float(kbcoeff)\n",
    "\n",
    "fkb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read coefficients of subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.374998Z",
     "start_time": "2021-10-19T03:53:09.363668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read configcoeff.out\n",
    "configcoef = {}\n",
    "\n",
    "with open('configcoef.out','r') as fsubmult:\n",
    "    _ = next(fsubmult) #ignore first line\n",
    "    temp_submult = fsubmult.read() \n",
    "    temp_submult = pattern2.split(temp_submult) #split lines into blocks separated by 2 empty lines\n",
    "    \n",
    "for idx, submult in enumerate(temp_submult):\n",
    "    submult = submult.split('\\n') #split into number of subclusters \n",
    "    while(\"\" in submult) :\n",
    "        submult.remove(\"\") #remove empty blocks\n",
    "    configcoef[idx] = list(map(float,submult[1:])) #also ignore 1st line of each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.383936Z",
     "start_time": "2021-10-19T03:53:09.377944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1.0],\n",
       " 1: [1.0, 1.0],\n",
       " 2: [1.0, 2.0, 1.0],\n",
       " 3: [1.0, 2.0, 1.0],\n",
       " 4: [1.0, 2.0, 1.0, 1.0, 2.0, 1.0],\n",
       " 5: [1.0, 4.0, 2.0, 4.0, 4.0, 1.0],\n",
       " 6: [0.0, -1.0, 1.0, 1.0, -1.0, 1.0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read V-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.395720Z",
     "start_time": "2021-10-19T03:53:09.387824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read vmat.out\n",
    "vmat = {}\n",
    "with open('vmat.out') as fvmat:\n",
    "    _ = next(fvmat) #ignore first lie\n",
    "    temp_vmat = fvmat.read()\n",
    "    temp_vmat = pattern2.split(temp_vmat) #split by 2 empty lines i.e. maxclusters\n",
    "    \n",
    "    while(\"\" in temp_vmat):\n",
    "        temp_vmat.remove(\"\") #remove empty blocks\n",
    "    \n",
    "    for clus_idx, mat in enumerate(temp_vmat):\n",
    "        mat = mat.split('\\n') #split by 1 empty line i.e. subclusters\n",
    "        mat_float = np.empty(list(map(int, mat[0].split(' '))))\n",
    "        for idx, row in enumerate(mat[1:]): #ignore first line\n",
    "            mat_float[idx] = list(map(float,row.split(' ')[:-1]))\n",
    "        \n",
    "        vmat[clus_idx] = mat_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.414292Z",
     "start_time": "2021-10-19T03:53:09.398535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[1., 0., 0., 0., 0., 0.]]),\n",
       " 1: array([[ 0.5, -0.5,  0. ,  0. ,  0. ,  0. ],\n",
       "        [ 0.5,  0.5,  0. ,  0. ,  0. ,  0. ]]),\n",
       " 2: array([[ 0.25, -0.5 ,  0.25,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.  , -0.25,  0.  ,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.5 ,  0.25,  0.  ,  0.  ,  0.  ]]),\n",
       " 3: array([[ 0.25, -0.5 ,  0.  ,  0.25,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.  ,  0.  , -0.25,  0.  ,  0.  ],\n",
       "        [ 0.25,  0.5 ,  0.  ,  0.25,  0.  ,  0.  ]]),\n",
       " 4: array([[ 0.12, -0.38,  0.25,  0.12, -0.12,  0.  ],\n",
       "        [ 0.12, -0.12,  0.  , -0.12,  0.12,  0.  ],\n",
       "        [ 0.12,  0.12, -0.25,  0.12, -0.12,  0.  ],\n",
       "        [ 0.12, -0.12, -0.25,  0.12,  0.12,  0.  ],\n",
       "        [ 0.12,  0.12,  0.  , -0.12, -0.12,  0.  ],\n",
       "        [ 0.12,  0.38,  0.25,  0.12,  0.12,  0.  ]]),\n",
       " 5: array([[ 0.06, -0.25,  0.25,  0.12, -0.25,  0.06],\n",
       "        [ 0.06, -0.12,  0.  ,  0.  ,  0.12, -0.06],\n",
       "        [ 0.06,  0.  , -0.25,  0.12,  0.  ,  0.06],\n",
       "        [ 0.06,  0.  ,  0.  , -0.12,  0.  ,  0.06],\n",
       "        [ 0.06,  0.12,  0.  ,  0.  , -0.12, -0.06],\n",
       "        [ 0.06,  0.25,  0.25,  0.12,  0.25,  0.06]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ECI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.425080Z",
     "start_time": "2021-10-19T03:53:09.416305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read eci\n",
    "eci = {}\n",
    "\n",
    "with open('eci.out') as feci:\n",
    "    _ = next(feci) #Ignore first line\n",
    "    temp_eci = feci.read()\n",
    "    temp_eci = temp_eci.split('\\n') #split by line\n",
    "\n",
    "for idx, eci_val in enumerate(temp_eci):\n",
    "    if eci_val == '':\n",
    "        continue\n",
    "    eci[idx] = float(eci_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.435456Z",
     "start_time": "2021-10-19T03:53:09.427259Z"
    }
   },
   "outputs": [],
   "source": [
    "#In this notebook ECI's are declared manually\n",
    "eci_2 = 0.01\n",
    "eci_3 = 0.05*eci_2\n",
    "eci = {0: 0, 1: 0.0, 2: eci_2, 3: eci_3, 4: 0, 5: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.449845Z",
     "start_time": "2021-10-19T03:53:09.437575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0.0, 2: 0.01, 3: 0.0005, 4: 0, 5: 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.465316Z",
     "start_time": "2021-10-19T03:53:09.452005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   -0.25  0.9   0.46  0.2  -0.69]\n"
     ]
    }
   ],
   "source": [
    "corrs1 = np.array([1., 1., 1., 1., 1., 1.]) #Pure B\n",
    "corrs0 = np.array([1., -1., 1., 1., -1., 1.]) #Pure A\n",
    "corrssqs = np.array([1.    , 0.0   , 0.25,0.25  , 0.125 , 0.0625]) # AB - sqs\n",
    "corrsrand = np.array([1.   , *np.random.uniform(-1, 1, 5)]) \n",
    "T = 1/kB\n",
    "print(corrsrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.478006Z",
     "start_time": "2021-10-19T03:53:09.467049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[0.5 0.5]\n",
      "[0.31 0.19 0.31]\n",
      "[0.31 0.19 0.31]\n",
      "[0.2  0.11 0.08 0.11 0.08 0.23]\n",
      "[0.13 0.07 0.04 0.04 0.04 0.19]\n"
     ]
    }
   ],
   "source": [
    "for cluster_idx, _ in clusters.items():\n",
    "    rho = np.matmul(vmat[cluster_idx],corrssqs)\n",
    "    print(rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setting up Log Absolute functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Set up F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.489541Z",
     "start_time": "2021-10-19T03:53:09.481181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def F(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    F = H + kB*T*SUM(rho * log(rho))\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_corrsum(vmat,corrs):\n",
    "        assert len(vmat) == len(corrs)\n",
    "        corrsum = np.inner(vmat,corrs)\n",
    "        if corrsum == 0:\n",
    "            return 0\n",
    "            #corrsum = np.finfo(float).tiny\n",
    "\n",
    "        return corrsum * math.log(np.abs(corrsum))\n",
    "    \n",
    "    def per_cluster_sum(corrs,vmat,configcoef):\n",
    "        config_sum = np.sum([coef * get_corrsum(vmat[config_idx],corrs) for config_idx, coef in enumerate(configcoef)\n",
    "                            ])\n",
    "                      \n",
    "        return config_sum\n",
    "    \n",
    "    H = np.sum([cluster['mult']*eci[cluster_idx]*corrs[cluster_idx] \n",
    "                for cluster_idx, cluster in clusters.items()\n",
    "               ])\n",
    "    \n",
    "    S = np.sum([kb[cluster_idx]*per_cluster_sum(corrs,\n",
    "                                                vmat[cluster_idx],\n",
    "                                                configcoef[cluster_idx],)\n",
    "                for cluster_idx in clusters.keys()\n",
    "               ])\n",
    "    \n",
    "    return H + kB*T*S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.504392Z",
     "start_time": "2021-10-19T03:53:09.492241Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0: 0.04 -- Corrs 1: 0.04 -- Corrs Rand: -1.65 -- Corrs SQS: -2.62\n"
     ]
    }
   ],
   "source": [
    "f0 = F(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f1 = F(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "fsqs = F(corrssqs, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "frand = F(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0: {f0:.2f} -- Corrs 1: {f1:.2f} -- Corrs Rand: {frand:.2f} -- Corrs SQS: {fsqs:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Set up F Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.516603Z",
     "start_time": "2021-10-19T03:53:09.506401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def F_jacobian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    Vector representation gradient of F with Corrs\n",
    "    [dF/dcorr0, dF/dcorr1, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_kth_elem_jac(corrs, vmat, kb, clusters, configs, configcoef,corr_idx):\n",
    "        \n",
    "        dS_k = np.sum([kb[cluster_idx]*per_cluster_sum_jac(corrs,\n",
    "                                                           vmat[cluster_idx],\n",
    "                                                           configcoef[cluster_idx],\n",
    "                                                           corr_idx,) \n",
    "                       for cluster_idx in clusters.keys()\n",
    "                      ])\n",
    "        return dS_k\n",
    "    \n",
    "    def get_corrsum_jac(vmat,corrs):\n",
    "        assert len(vmat) == len(corrs)\n",
    "        corrsum = np.inner(vmat,corrs)\n",
    "        if corrsum == 0:\n",
    "            return np.NINF\n",
    "            #corrsum = np.finfo(float).tiny\n",
    "\n",
    "        return 1 + math.log(np.abs(corrsum))\n",
    "\n",
    "    def per_cluster_sum_jac(corrs,vmat,configcoef,corr_idx):\n",
    "        \n",
    "        config_sum = np.sum([coef * vmat[config_idx][corr_idx] * get_corrsum_jac(vmat[config_idx],corrs) for config_idx, coef in enumerate(configcoef)\n",
    "                            ])\n",
    "\n",
    "        \n",
    "        return config_sum\n",
    "    \n",
    "    dH = np.array([cluster['mult']*eci[cluster_idx] for cluster_idx, cluster in clusters.items()])\n",
    "    \n",
    "    dS = np.array([get_kth_elem_jac(corrs, vmat, kb, clusters, configs, configcoef,corr_idx) for corr_idx, _ in enumerate(corrs)])\n",
    "    \n",
    "    return dH + kB*T*dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.569252Z",
     "start_time": "2021-10-19T03:53:09.518193Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0: [nan nan nan nan nan nan] \n",
      "Corrs 1: [nan nan nan nan nan nan] \n",
      "Corrs Rand [-1.76 -0.5   0.66  0.73  0.25  0.  ] \n",
      "Corrs SQS [-1.9  -0.1   0.61  0.42  0.23 -0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayan/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "<ipython-input-17-873df088579e>:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  config_sum = np.sum([coef * vmat[config_idx][corr_idx] * get_corrsum_jac(vmat[config_idx],corrs) for config_idx, coef in enumerate(configcoef)\n"
     ]
    }
   ],
   "source": [
    "f_jaco0 = F_jacobian(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_jaco1 = F_jacobian(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_jacosqs = F_jacobian(corrssqs, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_jacorand = F_jacobian(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0: {f_jaco0} \\nCorrs 1: {f_jaco1} \\nCorrs Rand {f_jacorand} \\nCorrs SQS {f_jacosqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Set up Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.579523Z",
     "start_time": "2021-10-19T03:53:09.570976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def F_hessian(corrs, vmat, kb, clusters, configs, configcoef,T,eci):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corrs - Correlations\n",
    "    vmat  - V-Matrix\n",
    "    clusters - Maximal Cluster Information (multiplicity, longest neighbor length, no. of points)\n",
    "    configs - Not used\n",
    "    configcoef - Coefficients of subclusters - array containing the coeff of each subcluster\n",
    "    T - Temperature\n",
    "    eci - ECI's\n",
    "    \n",
    "    Output:\n",
    "    Vector representation gradient of F with Corrs\n",
    "    [[d^2F/dcorr0 dcorr0, d^2F/dcorr0 dcorr1, ..., d^2F/dcorr0 dcorrn],\n",
    "     [d^2F/dcorr1 dcorr0, d^2F/dcorr1 dcorr1, ..., d^2F/dcorr1 dcorrn],\n",
    "     .\n",
    "     .\n",
    "     .\n",
    "     [d^2F/dcorrn dcorr0, d^2F/dcorrn dcorr1, ..., d^2F/dcorrn dcorrn],\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_corrsum_hess(vmat,corrs):\n",
    "        assert len(vmat) == len(corrs)\n",
    "        \n",
    "        corrsum = np.inner(vmat,corrs)\n",
    "        if corrsum == 0:\n",
    "            return 1/np.PINF\n",
    "            #corrsum = np.finfo(float).tiny\n",
    "\n",
    "        return corrsum\n",
    "    \n",
    "    def get_config_val(corrs,vmat,configcoef,corr_idx_1,corr_idx_2):\n",
    "        \n",
    "        config_val = np.sum([coef * vmat[config_idx][corr_idx_1] * vmat[config_idx][corr_idx_2] / get_corrsum_hess(vmat[config_idx],corrs) \n",
    "                             for config_idx, coef in enumerate(configcoef)\n",
    "                            ])\n",
    "        \n",
    "        return config_val\n",
    "    \n",
    "    def get_hessian_elem(corrs, vmat, kb, clusters, configs, configcoef,T,eci,corr_idx_1,corr_idx_2):\n",
    "        \n",
    "        hess_elem = np.sum([kb[cluster_idx] * get_config_val(corrs,\n",
    "                                                             vmat[cluster_idx],\n",
    "                                                             configcoef[cluster_idx],\n",
    "                                                             corr_idx_1,\n",
    "                                                             corr_idx_2\n",
    "                                                            ) \n",
    "                            for cluster_idx in clusters.keys()\n",
    "                           ])\n",
    "        return hess_elem\n",
    "    \n",
    "    d2F = np.empty([len(corrs),len(corrs)])\n",
    "    \n",
    "    d2F = np.array([[get_hessian_elem(corrs, vmat, kb, clusters, configs, configcoef, T, eci, corr_idx_1, corr_idx_2) for corr_idx_2, _ in enumerate(corrs)] for corr_idx_1, _ in enumerate(corrs)])\n",
    "    \n",
    "    return d2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.622472Z",
     "start_time": "2021-10-19T03:53:09.582574Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs 0:\n",
      " [[nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]] \n",
      " Corrs 1:\n",
      " [[nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]]\n",
      " Corrs Rand:\n",
      " [[ 1.42 -0.74 -2.66  2.25  0.85 -0.85]\n",
      " [-0.74 -1.12 -2.52  4.13  2.61 -0.43]\n",
      " [-2.66 -2.52  0.67  2.44  2.59  0.31]\n",
      " [ 2.25  4.13  2.44 -4.58 -3.9   0.76]\n",
      " [ 0.85  2.61  2.59 -3.9  -2.1   0.44]\n",
      " [-0.85 -0.43  0.31  0.76  0.44 -0.02]] \n",
      " Corrs SQS:\n",
      " [[ 1.25  0.13 -0.55 -0.41 -0.16  0.14]\n",
      " [ 0.13  2.99  0.02  0.01 -0.89 -0.35]\n",
      " [-0.55  0.02  3.49 -0.97 -0.25 -0.69]\n",
      " [-0.41  0.01 -0.97  2.76 -0.13 -0.34]\n",
      " [-0.16 -0.89 -0.25 -0.13  1.94  0.27]\n",
      " [ 0.14 -0.35 -0.69 -0.34  0.27  1.29]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-d4a292fd34bd>:35: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  config_val = np.sum([coef * vmat[config_idx][corr_idx_1] * vmat[config_idx][corr_idx_2] / get_corrsum_hess(vmat[config_idx],corrs)\n",
      "<ipython-input-19-d4a292fd34bd>:35: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  config_val = np.sum([coef * vmat[config_idx][corr_idx_1] * vmat[config_idx][corr_idx_2] / get_corrsum_hess(vmat[config_idx],corrs)\n"
     ]
    }
   ],
   "source": [
    "f_hess0 = F_hessian(corrs0, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hess1 = F_hessian(corrs1, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hessrand = F_hessian(corrsrand, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "f_hesssqs = F_hessian(corrssqs, vmat, kb, clusters, configs, configcoef,T,eci)\n",
    "print(f\"Corrs 0:\\n {f_hess0} \\n Corrs 1:\\n {f_hess1}\\n Corrs Rand:\\n {f_hessrand} \\n Corrs SQS:\\n {f_hesssqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T03:53:09.640676Z",
     "start_time": "2021-10-19T03:53:09.625058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def constraint_rhos_sum(corrs, vmat, clusters, configcoef,):\n",
    "    \"\"\"\n",
    "    Constraints the sum of each rho. As of now, it's done in a weird way, where the total sum of the array:\n",
    "    [1 - sum(rho), .... ] is constrained to sum to 0. This along with the constraint that each rho is between\n",
    "    0 and 1, seems to make it work. I think that by the this might be a redundant constraint as well.\n",
    "    \"\"\"\n",
    "    rho_sum = []\n",
    "\n",
    "    def clus_prob(cluster_idx):\n",
    "        rho = np.matmul(vmat[cluster_idx],corrs)\n",
    "        return rho\n",
    "    \n",
    "    for cluster_idx, _ in clusters.items():\n",
    "        rho = clus_prob(cluster_idx)\n",
    "        rho_sum.append(np.sum(configcoef[cluster_idx]*rho))\n",
    "    \n",
    "    return np.sum(1 - np.array(rho_sum))\n",
    "\n",
    "def constraint_singlet(corrs,FIXED_CORR_1):\n",
    "    \"\"\"\n",
    "    constrains the 1-pt correlation:\n",
    "    corrs[1] = FIXED_CORR_1\n",
    "    \"\"\"\n",
    "    return corrs[1] - FIXED_CORR_1   \n",
    "\n",
    "def constraint_zero(corrs):\n",
    "    \"\"\"\n",
    "    constrains the 1-pt correlation:\n",
    "    corrs[0] = 1\n",
    "    \"\"\"\n",
    "    return 1 - corrs[0]\n",
    "\n",
    "def constraint_NN(corrs,FIXED_CORR_2):\n",
    "    \"\"\"\n",
    "    constrains the 2-pt correlation:\n",
    "    corrs[2] = FIXED_CORR_2\n",
    "    \"\"\"\n",
    "    return corrs[2] - FIXED_CORR_2 \n",
    "\n",
    "class MyBounds:\n",
    "    \"\"\"\n",
    "    Class to constrain the trial correlations of Basin Hopping\n",
    "    \"\"\"\n",
    "    def __init__(self,xmax=[1]*6, xmin=[-1]*6,):\n",
    "        self.xmax = np.array(xmax)\n",
    "        self.xmin = np.array(xmin)\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        x = kwargs[\"x_new\"]\n",
    "        tmax = bool(np.all(x <= self.xmax))\n",
    "        tmin = bool(np.all(x >= self.xmin))\n",
    "\n",
    "        return tmax and tmin\n",
    "    \n",
    "class MyTakeStep:\n",
    "    \n",
    "    def __init__(self, vmat, clusters, stepsize=0.1):\n",
    "        self.stepsize = stepsize\n",
    "        self.vmat = vmat\n",
    "        self.clusters = clusters\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        s = self.stepsize\n",
    "        \n",
    "        validcorr = np.ones(len(self.clusters), dtype=bool)\n",
    "        \n",
    "        for _ in iter(int,1):\n",
    "            x_trial = x + self.rng.uniform(-s, s, x.shape)\n",
    "            for cluster_idx, _ in self.clusters.items():\n",
    "                rho = np.matmul(self.vmat[cluster_idx],x_trial)\n",
    "                validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "            if bool(np.all(validcorr)):\n",
    "                break\n",
    "            \n",
    "        return x_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:51:42.766051Z",
     "start_time": "2021-10-13T18:51:42.760931Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linspace(0, 500, num=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-19T03:53:31.319Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163cf627f1204cffabad9556323edf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500c7c280d914e40913ed83d59822edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayan/.local/lib/python3.8/site-packages/scipy/optimize/_trustregion_constr/projections.py:181: UserWarning: Singular Jacobian matrix. Using SVD decomposition to perform the factorizations.\n",
      "  warn('Singular Jacobian matrix. Using SVD decomposition to ' +\n",
      "/home/sayan/.local/lib/python3.8/site-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found new minimum for x:0, T:0.0 fun: -0.02570492425917527\n",
      "Current minimum correlations: [ 1.   -0.   -0.67  0.64  0.    0.38]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.040919013325830034\n",
      "Current minimum correlations: [ 1.   -0.   -1.   -0.7   0.   -0.14]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.0412812237336417\n",
      "Current minimum correlations: [ 1.    0.   -1.   -0.91 -0.   -0.12]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.041352589431319406\n",
      "Current minimum correlations: [ 1.   -0.   -1.   -0.95  0.1   0.6 ]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.041461920823634146\n",
      "Current minimum correlations: [ 1.   -0.   -1.   -0.99  0.    0.2 ]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.041466498792397034\n",
      "Current minimum correlations: [ 1.   -0.   -1.   -0.99  0.   -0.14]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.04147095067225223\n",
      "Current minimum correlations: [ 1.   -0.   -1.   -0.99  0.    0.47]\n",
      "Found new minimum for x:0, T:0.0 fun: -0.041490942573939975\n",
      "Current minimum correlations: [ 1.    0.   -1.   -1.   -0.    0.33]\n"
     ]
    }
   ],
   "source": [
    "results_uniform = pd.DataFrame(columns = ['T', '1-point_corr', 'F','corrs'])\n",
    "NUM_TRIALS = 100\n",
    "MAX_TEMP = 500\n",
    "\n",
    "for temp in tqdm(np.linspace(0, MAX_TEMP, num=11)):\n",
    "    for x in [0]:#tqdm(np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,9)):\n",
    "        FIXED_CORR_1 = x\n",
    "        \n",
    "        MIN_RES_VAL = 1e5 #random large number\n",
    "        rho_pair = np.array([None,None])\n",
    "        #corrs0 = np.array([1, *np.random.uniform(-1, 1, 2)])\n",
    "        MIN_RES = corrs0\n",
    "\n",
    "        linear_constraints = []\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            \n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1,*[-1]*(len(clusters)-2)],\n",
    "                              [1, FIXED_CORR_1,*[1]*(len(clusters)-2)]\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 3000,\n",
    "                   'xtol'    : 1e-15,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        \n",
    "        for _ in tqdm(range(NUM_TRIALS)):\n",
    "            \n",
    "            for _ in iter(int,1): #infinite loop till a valid starting correlations are found\n",
    "                corrs0 = np.array([1, x, *np.random.uniform(-1, 1, len(clusters)-2)])\n",
    "                validcorr = np.ones(len(clusters), dtype=bool)\n",
    "        \n",
    "                for cluster_idx, _ in clusters.items():\n",
    "                    rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "                    validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "                \n",
    "                if bool(np.all(validcorr)):\n",
    "                    break   \n",
    "                \n",
    "            res = minimize(F,\n",
    "                           corrs0,\n",
    "                           method='trust-constr',\n",
    "                           args=(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                           options=options,\n",
    "                           #jac='3-point',\n",
    "                           #hess=BFGS(),\n",
    "                           jac=F_jacobian,\n",
    "                           hess=F_hessian,\n",
    "                           constraints=[#*linear_constraints, \n",
    "                                        {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1]},\n",
    "                                        {'fun': constraint_zero, 'type':'eq',},\n",
    "                                       ],\n",
    "                           bounds=bounds_corrs,\n",
    "                          )\n",
    "            \n",
    "            if res.fun < MIN_RES_VAL:\n",
    "                MIN_RES = res\n",
    "                MIN_RES_VAL = res.fun\n",
    "                print(f\"Found new minimum for x:{x}, T:{temp} fun: {MIN_RES_VAL}\")\n",
    "                print(f'Current minimum correlations: {res.x}')\n",
    "                #for cluster_idx in clusters.keys():\n",
    "                #    print(np.matmul(vmat[cluster_idx],res.x))\n",
    "        \n",
    "            \n",
    "        for cluster_idx in clusters.keys():\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "        \n",
    "        results_uniform = results_uniform.append({'T' : temp, \n",
    "                                           '1-point_corr' : x, \n",
    "                                           'F' : MIN_RES.fun, \n",
    "                                           'corrs': MIN_RES.x,\n",
    "                                          }, \n",
    "                                          ignore_index = True\n",
    "                                         )\n",
    "        \n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H:%M\")\n",
    "results_uniform.to_pickle(f'results/uni_{eci[2]},{eci[3]}_{MAX_TEMP}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:28:44.498131Z",
     "start_time": "2021-10-18T22:28:44.489156Z"
    }
   },
   "outputs": [],
   "source": [
    "eci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Phase Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T01:20:02.754398Z",
     "start_time": "2021-10-13T01:20:02.648016Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for T in results_uniform['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_uniform[results_uniform['T'] == T]['1-point_corr'],\n",
    "                             y = results_uniform[results_uniform['T'] == T]['F'],\n",
    "                             mode='markers+lines',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F vs 1-point Corr\",\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"F\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Pair Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T00:46:30.800874Z",
     "start_time": "2021-10-19T00:46:30.740422Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = results_uniform[results_uniform['1-point_corr'] == 0.0]['T'],\n",
    "                         y = results_uniform[results_uniform['1-point_corr'] == 0.0]['corrs'].str[2],\n",
    "                         mode='markers+lines',\n",
    "                        )\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"T vs 2-point Corr\",\n",
    "    xaxis_title=\"T\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T18:58:05.529681Z",
     "start_time": "2021-09-12T18:58:05.511796Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Basin Hopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Defining the Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def basin_hopping_callback(corrs, G, accept):\n",
    "    \"\"\"\n",
    "    Function to print diagnostic while fitting. \n",
    "    Not being used right now\n",
    "    \"\"\"\n",
    "    if accept == True:\n",
    "        clear_output(wait=True)\n",
    "        print(f'1-pt Correlation: {corrs[1]:.2f}')\n",
    "        print(f'Concentation: {(corrs[1] - (-1))/(1 - (-1)):.2f}')\n",
    "        print(f'New Minima found --> G: {G:.2f}')\n",
    "        print('Current Rho:')\n",
    "        for cluster_idx in clusters:\n",
    "            print(np.matmul(vmat[cluster_idx],corrs))\n",
    "        print(\"===========================\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Optimisation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T18:11:15.585712Z",
     "start_time": "2021-09-16T17:29:34.718589Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_basinhopping = pd.DataFrame(columns = ['T', '1-point_corr', 'F','corrs'])\n",
    "MAX_TEMP = 500\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H-%M\")\n",
    "\n",
    "for temp in tqdm(np.linspace(0, MAX_TEMP, num=21)):\n",
    "    for x in [0]:#tqdm(np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,9)):\n",
    "\n",
    "        FIXED_CORR_1 = x\n",
    "\n",
    "        rho_pair = np.array([None,None])\n",
    "        \n",
    "        #Make a first guess\n",
    "        for _ in iter(int,1): #infinite loop till a valid starting correlations are found\n",
    "            corrs0 = np.array([1, x, *np.random.uniform(-1, 1, 4)])\n",
    "            validcorr = np.ones(len(clusters), dtype=bool)\n",
    "\n",
    "            for cluster_idx, _ in clusters.items():\n",
    "                rho = np.matmul(vmat[cluster_idx],corrs0)\n",
    "                validcorr[cluster_idx] = np.all(rho >= 0)\n",
    "\n",
    "            if bool(np.all(validcorr)):\n",
    "                break   \n",
    "        \n",
    "        #corrs0 = np.array([1, x, *np.random.uniform(-1, 1, 4)])\n",
    "        linear_constraints = []\n",
    "\n",
    "        #Linear Constraint for each rho to be between 0 and 1\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "        \n",
    "        #Set bounds for the local minimisation \n",
    "        #limit correlations to [1, 1-pt, [-1,1], [-1,1], ...]\n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1,*[-1]*(len(clusters)-2)],\n",
    "                              [1, FIXED_CORR_1,*[1]*(len(clusters)-2)],\n",
    "                             )\n",
    "     \n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 5000,\n",
    "                   'xtol'    : 1e-9,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "        \n",
    "        minimizer_kwargs = {'args':(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                            'method': 'trust-constr',\n",
    "                            'options': options,\n",
    "                            'jac': F_jacobian, 'hess': F_hessian,\n",
    "                            'constraints' : [*linear_constraints, \n",
    "                                             {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1], 'hess': 0},\n",
    "                                             {'fun': constraint_zero, 'type':'eq','hess': 0},\n",
    "                                             #{'fun': constraint_rhos_sum, 'type': 'eq', 'args': [vmat, clusters, configcoef,]},\n",
    "                                            ],\n",
    "                            'bounds': bounds_corrs,\n",
    "                           }\n",
    "        \n",
    "        #Bounds for trial correlations\n",
    "        mybounds = MyBounds(xmax=[1, FIXED_CORR_1,*[1]*(len(clusters)-2)], \n",
    "                            xmin=[1, FIXED_CORR_1,*[-1]*(len(clusters)-2)]\n",
    "                           )\n",
    "        \n",
    "        mytakestep = MyTakeStep(vmat,\n",
    "                                clusters,\n",
    "                                stepsize=0.05\n",
    "                               )\n",
    "        \n",
    "        res = basinhopping(F, \n",
    "                           corrs0, #first guess\n",
    "                           niter=1000, #total num of iterations\n",
    "                           T=0.01, #temp for Metropolis MC trial search\n",
    "                           #stepsize=0.1,\n",
    "                           minimizer_kwargs=minimizer_kwargs,\n",
    "                           niter_success=10, #num iters to exit after no new minima found \n",
    "                           interval=5, #num iters to change step size\n",
    "                           disp=True,\n",
    "                           #accept_test=mybounds,\n",
    "                           take_step=mytakestep,\n",
    "                           #seed=42,\n",
    "                           #callback=basin_hopping_callback\n",
    "                          )\n",
    "\n",
    "        #Code to extract rhos and check if they sum them to 1 for sanity. Not used.\n",
    "        for cluster_idx in clusters.keys():\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "        \n",
    "        results_basinhopping = results_basinhopping.append({'T' : temp, \n",
    "                                                     '1-point_corr' : x, \n",
    "                                                     'F' : res.fun, \n",
    "                                                     'corrs': res.x,\n",
    "                                                    }, \n",
    "                                                    ignore_index = True\n",
    "                                                   )\n",
    "\n",
    "#save results\n",
    "results_basinhopping.to_pickle(f'results/bh_{eci[2]}_{MAX_TEMP}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Plot Phase Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:08:24.036119Z",
     "start_time": "2021-09-15T19:08:23.974295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for T in results_basinhopping['T'].unique():\n",
    "    fig.add_trace(go.Scatter(x = results_basinhopping[results_basinhopping['T'] == T]['1-point_corr'],\n",
    "                             y = results_basinhopping[results_basinhopping['T'] == T]['F'],\n",
    "                             mode='lines+markers',\n",
    "                             name=f'T = {T}',\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F vs 1-point Corr\",\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"F\",\n",
    "    legend_title=\"Temperature\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot Pair Correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T18:11:26.033136Z",
     "start_time": "2021-09-16T18:11:25.709737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = results_basinhopping[results_basinhopping['1-point_corr'] == 0.0]['T'],\n",
    "                         y = results_basinhopping[results_basinhopping['1-point_corr'] == 0.0]['corrs'].str[2],\n",
    "                         mode='markers+lines',\n",
    "                        )\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"T vs 2-point Corr\",\n",
    "    xaxis_title=\"T\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    template='seaborn'\n",
    ")\n",
    "#fig.update_traces(texttemplate='%{text:.2s}', textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fix 1-point and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:03:26.903822Z",
     "start_time": "2021-10-18T19:03:26.890428Z"
    },
    "code_folding": [
     0,
     4,
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def singlet_doublet_validpts(triangle = np.array([\n",
    "    [0, 0],\n",
    "    [-1, 1],\n",
    "    [1, 1],\n",
    "])):\n",
    "    \n",
    "    def uniform_triangle(u, v):\n",
    "        while True:\n",
    "            s = random.random()\n",
    "            t = random.random()\n",
    "            in_triangle = s + t <= 1\n",
    "            p = s * u + t * v if in_triangle else (1 - s) * u + (1 - t) * v\n",
    "            yield p\n",
    "\n",
    "    it = uniform_triangle(\n",
    "        triangle[1] - triangle[0],\n",
    "        triangle[2] - triangle[0],\n",
    "    )\n",
    "\n",
    "    points = np.array(list(itertools.islice(it, 0, 100)))\n",
    "    points += triangle[0]\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:03:27.316533Z",
     "start_time": "2021-10-18T19:03:27.296286Z"
    },
    "code_folding": [
     0,
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1, num = 10):\n",
    "    \n",
    "    def isInside(x, y, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1,):\n",
    "    \n",
    "        def area(x1, y1, x2, y2, x3, y3):\n",
    "            return abs((x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2))/2.0)\n",
    "\n",
    "        # Calculate area of triangle ABC\n",
    "        A = area(x1, y1, x2, y2, x3, y3)\n",
    "        # Calculate area of triangle PBC\n",
    "        A1 = area(x, y, x2, y2, x3, y3)\n",
    "        # Calculate area of triangle PAC\n",
    "        A2 = area(x1, y1, x, y, x3, y3)\n",
    "        # Calculate area of triangle PAB\n",
    "        A3 = area(x1, y1, x2, y2, x, y)\n",
    "        # Check if sum of A1, A2 and A3\n",
    "        # is same as A\n",
    "        if(A == A1 + A2 + A3):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "                                   \n",
    "\n",
    "    points = []\n",
    "    #xx, yy = np.meshgrid(xvalues, yvalues)\n",
    "    grid = np.meshgrid(xvalues, yvalues)\n",
    "    grid = np.vstack(list(map(np.ravel, grid))).T\n",
    "\n",
    "    for x, y in grid:\n",
    "        if isInside(x,y,x1=x1,y1=y1,x2=x2,y2=y2,x3=x3,y3=y3):\n",
    "            points.append([x,y])\n",
    "    \n",
    "    return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:03:31.502970Z",
     "start_time": "2021-10-18T19:03:31.299730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#singlet_doublet_pairs = singlet_doublet_validpts()\n",
    "num = 8\n",
    "xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "yvalues = np.linspace(0,1,num)\n",
    "singlet_doublet_pairs_1 = get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=1, x3=1, y3=1, num = num)\n",
    "\n",
    "xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "yvalues = np.linspace(0,-1,num)\n",
    "singlet_doublet_pairs_2 = get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=0, x2=-1, y2=-1, x3=1, y3=-1,num=num)\n",
    "\n",
    "xvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "yvalues = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,num)\n",
    "singlet_doublet_pairs_3 = get_uniform_traingular_points(xvalues, yvalues, x1=0, y1=-1, x2=-1, y2=1, x3=1, y3=1,num=num)\n",
    "singlet_doublet_pairs_3 = np.append(singlet_doublet_pairs_3,[[0,-1+np.finfo(float).eps]],axis=0)\n",
    "\n",
    "singlet_doublet_pairs = np.vstack([np.array(singlet_doublet_pairs_1),np.array(singlet_doublet_pairs_2)])\n",
    "plt.style.use('default')\n",
    "plt.scatter(singlet_doublet_pairs_3[:, 0], singlet_doublet_pairs_3[:, 1], s=5)\n",
    "print(len(singlet_doublet_pairs_3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T19:27:07.131374Z",
     "start_time": "2021-10-18T19:07:02.219079Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_blanket = pd.DataFrame(columns = ['T', '1-point_corr', '2-point_corr', 'F','corrs'])\n",
    "NUM_TRIALS = 50\n",
    "MAX_TEMP = 500\n",
    "\n",
    "#corrsrange = np.linspace(-1+np.finfo(float).eps,1-np.finfo(float).eps,21)\n",
    "#singlet_doublet_pairs = singlet_doublet_validpts()\n",
    "#singlet_doublet_pairs = get_uniform_traingular_points(num=20)\n",
    "print(f'Working ECIs: {eci}')\n",
    "for temp in tqdm(np.linspace(0, MAX_TEMP, num=11)):\n",
    "    for FIXED_CORR_1, FIXED_CORR_2 in tqdm(singlet_doublet_pairs_3):\n",
    "        \n",
    "        print('\\n=============================================================')\n",
    "        print(f'Working on 1-point corr: {FIXED_CORR_1:.4f}, NN corr: {FIXED_CORR_2:.4f}, Temp: {temp}')\n",
    "        print('=============================================================\\n')\n",
    "\n",
    "        MIN_RES_VAL = 1e5 #random large number\n",
    "        MIN_RES = corrs0\n",
    "\n",
    "        linear_constraints = []\n",
    "\n",
    "        for cluster_idx, _ in clusters.items():\n",
    "\n",
    "            if cluster_idx == 0:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [1]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "            else:\n",
    "                linear_constraints.append(LinearConstraint(vmat[cluster_idx],\n",
    "                                                           [0]*len(configcoef[cluster_idx]),\n",
    "                                                           [1]*len(configcoef[cluster_idx])))\n",
    "\n",
    "        bounds_corrs = Bounds([1, FIXED_CORR_1, FIXED_CORR_2, *[-1]*(len(clusters)-3)],\n",
    "                              [1, FIXED_CORR_1, FIXED_CORR_2, *[1]*(len(clusters)-3)]\n",
    "                             )\n",
    "\n",
    "        options = {'verbose' : 0,\n",
    "                   'maxiter' : 3000,\n",
    "                   'xtol'    : 1e-15,\n",
    "                   'initial_constr_penalty' : 10,\n",
    "                  }\n",
    "\n",
    "        for _ in tqdm(range(NUM_TRIALS)):\n",
    "\n",
    "            corrs0 = np.array([1, FIXED_CORR_1, FIXED_CORR_2, *np.random.uniform(-1, 1, len(clusters)-3)])\n",
    "\n",
    "            res = minimize(F,\n",
    "                           corrs0,\n",
    "                           method='trust-constr',\n",
    "                           args=(vmat, kb, clusters, configs, configcoef,temp,eci),\n",
    "                           options=options,\n",
    "                           #jac='3-point',\n",
    "                           #hess=BFGS(),\n",
    "                           jac=F_jacobian,\n",
    "                           hess=F_hessian,\n",
    "                           constraints=[*linear_constraints, \n",
    "                                        {'fun': constraint_singlet, 'type': 'eq', 'args': [FIXED_CORR_1]},\n",
    "                                        {'fun': constraint_zero, 'type':'eq',},\n",
    "                                        {'fun': constraint_NN, 'type':'eq','args':[FIXED_CORR_2]}\n",
    "                                       ],\n",
    "                           bounds=bounds_corrs,\n",
    "                          )\n",
    "\n",
    "            if res.fun < MIN_RES_VAL:\n",
    "                MIN_RES = res\n",
    "                MIN_RES_VAL = res.fun\n",
    "                print(f\"Found new minimum for Corr1:{FIXED_CORR_1:.4f}, Corr2:{FIXED_CORR_2:.4f} fun: {MIN_RES_VAL:.15f}\")\n",
    "                print(f'Current minimum correlations: {res.x}')\n",
    "#                     for cluster_idx, _ in clusters.items():\n",
    "#                         rho = np.matmul(vmat[cluster_idx],res.x)\n",
    "#                         print(rho)\n",
    "\n",
    "\n",
    "        break_next = False\n",
    "        for cluster_idx in clusters.keys():\n",
    "            try:\n",
    "                assert np.isclose(np.inner(configcoef[cluster_idx],np.matmul(vmat[cluster_idx],res.x)),1.0)\n",
    "            except AssertionError:\n",
    "                break_next = True\n",
    "        if break_next:\n",
    "            print('No valid solution found')\n",
    "            continue\n",
    "\n",
    "        results_blanket = results_blanket.append({'T' : temp, \n",
    "                                           '1-point_corr' : FIXED_CORR_1,\n",
    "                                           '2-point_corr' : FIXED_CORR_2,\n",
    "                                           'F' : MIN_RES.fun, \n",
    "                                           'corrs': MIN_RES.x,\n",
    "                                          }, \n",
    "                                          ignore_index = True\n",
    "                                         )\n",
    "        #print(results_blanket)\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%d-%m-%H:%M\")\n",
    "results_blanket.to_pickle(f'results/blanket_{eci[2]}_{eci[3]}_{MAX_TEMP}.pickle')\n",
    "results_blanket.to_pickle('temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:02:48.014002Z",
     "start_time": "2021-10-09T00:02:48.006527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "now = now.strftime(\"%d\")\n",
    "results_blanket.to_pickle(f'results/blanket_{eci[2]}_{MAX_TEMP}_{now}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:02:59.166506Z",
     "start_time": "2021-10-09T00:02:59.153758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_blanket['T'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:03:01.178654Z",
     "start_time": "2021-10-09T00:03:00.986579Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = 1000\n",
    "results_blanket[\n",
    "                (results_blanket['T'] == temp)\n",
    "               ]\n",
    "\n",
    "Xs = results_blanket[results_blanket['T'] == temp]['1-point_corr'].values\n",
    "Ys = results_blanket[results_blanket['T'] == temp]['2-point_corr'].values\n",
    "Zs = results_blanket[results_blanket['T'] == temp]['F'].values\n",
    "\n",
    "fig, ax2 = plt.subplots(nrows=1,figsize=(8, 6), dpi=100)\n",
    "plt.style.use('ggplot')\n",
    "ax2.set_title(f'T = {temp}')\n",
    "ax2.tricontour(Xs, Ys, Zs, levels=10, linewidths=0.5,)\n",
    "cntr2 = ax2.tricontourf(Xs, Ys, Zs, levels=14,)\n",
    "\n",
    "fig.colorbar(cntr2, ax=ax2,label='F')\n",
    "ax2.plot(Xs, Ys, 'ko', ms=3)\n",
    "ax2.set(xlim=(-1, 1), ylim=(-1, 1))\n",
    "\n",
    "plt.xlabel(\"1-point Correlation\")\n",
    "plt.ylabel(\"2-point NN Correlation\")\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T22:35:45.505484Z",
     "start_time": "2021-10-07T22:35:45.497025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T11:05:11.570692Z",
     "start_time": "2021-09-28T11:05:11.563006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_blanket['T'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T11:12:56.455252Z",
     "start_time": "2021-09-28T11:12:56.288308Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "#fig.add_trace(traces)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"1-point Corr\",\n",
    "    yaxis_title=\"2-point Corr\",\n",
    "    template='ggplot2',\n",
    "    legend_title=\"Legend Title\",\n",
    ")\n",
    "for temp in [0,150,200,300,450]:#[0.,  50., 100., 150., 200., 250., 300., 350., 400., 450., 500.]:\n",
    "    Xs = results_blanket[results_blanket['T'] == temp]['1-point_corr'].values\n",
    "    Ys = results_blanket[results_blanket['T'] == temp]['2-point_corr'].values\n",
    "    Zs = results_blanket[results_blanket['T'] == temp]['F'].values\n",
    "    points2D = np.vstack([Xs,Ys]).T\n",
    "    tri = Delaunay(points2D)\n",
    "\n",
    "    simplices = tri.simplices\n",
    "    temp_trace = ff.create_trisurf(x=Xs, y=Ys, z=Zs,\n",
    "                                   simplices=simplices,\n",
    "                                   title=\"F vs singlet-NN-corrlations\",\n",
    "                                   show_colorbar=False,\n",
    "                                   width=1000,\n",
    "                                   height=1000,\n",
    "                                   #aspectratio=dict(x=1, y=1, z=10)\n",
    "                                  )\n",
    "    fig.add_traces([temp_trace.data[0]])\n",
    "\n",
    "fig.layout.scene.xaxis.title='1-point Correlation'\n",
    "fig.layout.scene.yaxis.title='2-point Correlation'\n",
    "fig.layout.scene.zaxis.title='F'\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=750,\n",
    "    height=750,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T15:59:52.236471Z",
     "start_time": "2021-09-27T15:59:52.231114Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig.layout.legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T22:05:12.467587Z",
     "start_time": "2021-09-25T22:05:12.461139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# for T in results_blanket['T'].unique():\n",
    "#     fig.add_trace(go.Scatter(x = results_blanket[results_blanket['T'] == T]['2-point_corr'],\n",
    "#                              y = results_blanket[results_blanket['T'] == T]['F'],\n",
    "#                              mode='markers+lines',\n",
    "#                              name=f\"T = {T}: ECI = {results_blanket.iloc[0]['F']/4:.2f}\",\n",
    "#                             )\n",
    "#                  )\n",
    "    \n",
    "# fig.update_layout(\n",
    "#     title=\"T vs 2-point Corr\",\n",
    "#     xaxis_title=\"2-point Corr\",\n",
    "#     yaxis_title=\"Energy\",\n",
    "#     template='seaborn'\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
